\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal,comsoc]{../sty/IEEEtran}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% copied from thesis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Packages für Formeln %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=green
            ]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

%% Packages
%\usepackage{tikz}
%\usetikzlibrary{shapes,arrows}
%\usetikzlibrary{positioning}
\usepackage{graphicx}
\usepackage{kantlipsum} %<- For dummy text
\usepackage{mwe} %<- For dummy images
\usepackage{multirow}
\usepackage{multicol}
%\usepackage{colortbl}
%\usepackage{todonotes}	
\newcounter{todocounter}
\newcommand{\todonum}[2][]
{\stepcounter{todocounter}\todo[inline,backgroundcolor=blue!10!white, #1]{\thetodocounter: #2}}

%% Zeilenabstand %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{setspace}
%\singlespacing        %% 1-zeilig (Standard)
%\onehalfspacing       %% 1,5-zeilig
%\doublespacing        %% 2-zeilig


%% Andere Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{a4wide} %%Kleinere Seitenränder = mehr Text pro Zeile.
\usepackage{fancyhdr} %%Fancy Kopf- und Fußzeilen
%\usepackage{longtable} %%Für Tabellen, die eine Seite überschreiten
\usepackage{lscape}
\usepackage{rotating} 
%\usepackage[htt]{hyphenat} %Trennung von Typewriter-Schriften
%\usepackage{listings}
%\usepackage{pstricks-add} --> This package generates problems with booktabs (toprule, etc.)
\usepackage[autostyle]{csquotes}
\usepackage{amsmath,bm}
\usepackage{array}
% Tabellen mit Center und left
\usepackage{tabularx,colortbl} % colored table background
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
% Table spacings
\newcommand\T{\rule{0pt}{2.5ex}\rule[-1.0ex]{0pt}{0pt}}
\newcommand\B{\rule[-1.0ex]{0pt}{0pt}}

\definecolor{slightgray}{gray}{.90} 
\usepackage{rotating}
\usepackage{hhline}



\newtheoremstyle{mytheoremstyle} % name
        {\topsep}                    % Space above
        {\topsep}                    % Space below
        {\itshape\fontfamily{ptm}\selectfont}                   % Body font, ptm is times new roma
        {}                           % Indent amount
        {\fontfamily{ptm}\selectfont\scshape\color{black}\bfseries}                   % Theorem head font
        {:}                          % Punctuation after theorem head
        {.5em}                       % Space after theorem head
        {}  % Theorem head spec (can be left empty, meaning ‘normal’)

  
          
\theoremstyle{mytheoremstyle}
\newtheorem{theorem}{Theorem}[section]

\theoremstyle{mytheoremstyle}
\newtheorem{corollary}{Corollary}[section]

\theoremstyle{mytheoremstyle}
\newtheorem{lemma}{Lemma}[section]

%\theoremstyle{mytheoremstyle}
%\let\definition\relax
\newtheorem{mydef}{Definition}%[section]

%%%%%%
%%%%%% Proof
    \makeatletter
    \renewenvironment{proof}[1][\proofname]{%
      \par\pushQED{\qed}\fontfamily{ptm}\selectfont%
      \topsep6\p@\@plus6\p@\relax
      \trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]%
      \ignorespaces
    }{%
      \popQED\endtrivlist\@endpefalse
    }
    \makeatother 
%%%%%%%%%%%%%%%%%

\newcommand{\eg}{e.g., }
\newcommand{\ie}{i.e., }
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%\usepackage{algorithmic}
\usepackage{times}
%\usepackage{mathptmx}
\usepackage{mathtools}
\usepackage[draft,nomargin,marginclue,footnote,silent]{fixme}
\setcounter{tocdepth}{2}

\makeatother
\usepackage{comment}
\newcommand{\Tau}{\mathrm{T}}
\usepackage[colorinlistoftodos]{todonotes}
\DeclareMathOperator*{\Max}{Max}
\DeclareMathOperator*{\Min}{Min}

\newcommand{\bigO}{\ensuremath{\mathcal{O}}}% big-O notation/symbol
\usepackage{subfigure}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{lipsum}
\setlist[itemize]{leftmargin=*}
%%%%%%%%%%%%%%%%%%%%%%%%%% copied from thesis
%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage[T1]{fontenc}% optional T1 font encoding



% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
% Do NOT use the amsbsy package under comsoc mode as that feature is
% already built into the Times Math font (newtxmath, mathtime, etc.).
% 
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% Select a Times math font under comsoc mode or else one will automatically
% be selected for you at the document start. This is required as Communications
% Society journals use a Times, not Computer Modern, math font.
%\usepackage[cmintegrals]{newtxmath}
\usepackage{txfonts}
% The freely available newtxmath package was written by Michael Sharpe and
% provides a feature rich Times math font. The cmintegrals option, which is
% the default under IEEEtran, is needed to get the correct style integral
% symbols used in Communications Society journals. Version 1.451, July 28,
% 2015 or later is recommended. Also, do *not* load the newtxtext.sty package
% as doing so would alter the main text font.
% http://www.ctan.org/pkg/newtx
%
% Alternatively, you can use the MathTime commercial fonts if you have them
% installed on your system:
%\usepackage{mtpro2}
%\usepackage{mt11p}
%\usepackage{mathtime}


%\usepackage{bm}
% The bm.sty package was written by David Carlisle and Frank Mittelbach.
% This package provides a \bm{} to produce bold math symbols.
% http://www.ctan.org/pkg/bm


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Versatile Robust Clustering of Ad Hoc Cognitive Radio Network}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Di~Li,~\IEEEmembership{Member,~IEEE,}
        Erwin~Fang,
        and James~Gross,~\IEEEmembership{Member,~IEEE}% <-this % stops a space
\thanks{D. Li was with RWTH Aachen university, Germany.}% <-this % stops a space
\thanks{Erwin. Fang is with ETH, Switzerland.}% <-this % stops a space
\thanks{J. Gross is with KTH Royal Institute of Technology, Sweden.}% <-this % stops a space
\thanks{Manuscript received xxxx xx, 20xx; revised xxxx xx, 20xx.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of xxxxx,~Vol.~xx, No.~x, xxxx~20xx}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Communications Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Cluster structure in cognitive radio networks facilitates cooperative spectrum sensing, routing and other functionalities.
The availability of unlicensed channels which are available for every member in a cluster decides the survival of that cluster from licensed users' influence.
Thus in order to be robust against licensed users, there should be more unlicensed channels in the clusters.
In the process of forming clusters, every secondary user needs to decide with whom to form a cluster, or which cluster to join.
Congestion game model is adopted to analyse this process, which not only contributes the algorithm design directly, but also provides guarantee of convergence into Nash Equilibrium and convergence speed.
%
Our proposed distributed clustering scheme outperforms the comparison scheme in terms of robustness against primary users, convergence speed and volume of control messages.
Furthermore, the proposed clustering solution is versatile to fulfil other requirements such like fast convergence and cluster size control.
Besides, we prove the clustering problem to be NP-hard, and also propose the centralized solution.
The extensive simulation supports our claims.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Cognitive Radio, Cluster, Robust, game theory, congestion game, distributed, centralised, size control.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\graphicspath{
{../figures/04_clutering/}
}


\section{Introduction}
\label{intro}
%CR concept
\todo[inline]{todo: polish section 6}
\IEEEPARstart{C}{ognitive} radio (CR) is a promising technology to solve the spectrum scarcity problem~\cite{Mitola}.
Unlicensed users access the spectrum allocated to them whenever there is information to be transmitted.
In contrast, unlicensed users can only access the licensed spectrum after validating the channel is unoccupied by licensed users.
This refers to the process of sensing a particular channel and verifying (with a previously specified probability of error) that it is not used by a primary user currently.
In this hierarchical spectrum access model~\cite{zhao_survey_DSA_2007}, the licensed users are also called primary users (PU), and the CR users are known as secondary users and constitute the cognitive radio networks (CRN).

%cluster
As to the operation of CRN, efficient spectrum sensing is identified to be critical to the success of cognitive radio networks~\cite{Sahai_FundamentalDesignTradeoffs2006}.
Cooperative spectrum sensing is able to effectively cope with noise uncertainty and channel fading, thus remarkably improves the sensing accuracy~\cite{coorperativeSensing_Akyildiz11}.
Collaborative sensing relies on the consensus of CR users within certain area, and decreases considerably the false sensing reports caused by fading and shadowing of reporting channel.
In this regard, clustering is regarded as an effective method in cooperative spectrum sensing~\cite{Sun07_clustering_spectrum_secsing, Zhao07}, as a cluster forms adjacent secondary users as a collectivity to perform spectrum sensing together.
%As a result, more accurate sensing result can be obtained by collaborative sensing, and the improvement on spectrum sensing decreases the interference originating from CR users to primary users, which is highly desirable.
%Also,  prevents CR users from using channels that are occupied by primary users. 
Clustering is also efficient to enable all CR devices within the same cluster to stop payload transmission on the operating channel and initiate the sensing process, so that all the CR users\footnote{The term $\textit{user}$ and $\textit{node}$ are used interchangeably in this paper, in particular, user is used when its networking or cognitive ability are discussed or stressed, and node is used when the network topology is discussed.} within the one cluster are able to vacate the channel swiftly when primary users are detected by at least one CR node residing in the cluster~\cite{willkomm08}.
With cluster structure, as CR users can be notified by cluster head (CH) or other cluster members about the possible collision, the possibility for them to interfere neighbouring clusters is reduced~\cite{centralizedSharing80222}. 
Clustering algorithm has also proposed to support routing in cognitive ad-hoc networks~\cite{Abbasi_survey_07}.

%%[crn for clustering] 
%%due to attenuation of signal propagation, primary users can only be detected by CR users when they locate closely to CR users.
%In cognitive radio networks, secondary users which locate closely with each other are possibly affected by the same group of primary users, so that the availability of licensed spectrum is similar to them, \ie certain channels are available on each of them.
%The similarity of available spectrum on a group of neighbouring CR nodes, along with the benefit of collaborative decision among multiple nodes, leads to clustering as an effective approach for many applications.

%[robustness issue for clustering]
The communication within a cluster is conducted in the spectrum which is available for every member in that cluster.
Usually there are multiple unlicensed channels available for all the members in a cluster, which are referred as \textit{common control channels} (CCC).
When one or several members can not use one certain CCC because primary users are detected to appear on that channel, this channel will be excluded from the set of CCCs, in particular, if this channel is the working channel, then all the cluster members switch to another channel in the set of CCCs.
In the context of CRN, as the activity of primary users is controlled by licensed operators which are generally not known to CR users, the availability of CCCs for the formed clusters  is totally decided by primary users' activity.
In other words, the availability of CCCs for clusters is passive and can not guaranteed.
%
In CRN, one cluster survives the influence of primary users when at least one CCC is available for that cluster.
As the channel occupation by primary users is assumed to be uncontrollable to the CR users, a cluster formed with more CCCs will survive with higher probability.
Thus the number of CCCs in one cluster indicates robustness of it when facing ungovernable influence from primary users.
%It is easy to see that forming clusters with different neighbours leads to different amount of CCCs in the clusters.
As a result, how to form the clusters plays an important role on the robustness of clusters in CRN.

%Furthermore, the clustering algorithm also determines the connectivity between several clusters which ultimately determines the robustness of the entire network on the respect of connectivity. 

To solely pursue cluster robustness against the primary users' activity, \ie to achieve more common channels within clusters, the ultimately best clustering strategy is ironically that each node constitutes one single node clusters.
%In that case, the common channels within cluster are the available channels available at that node's place.
Apparently this contradicts our motivation of proposing cluster in cognitive radio network.
This contradiction indicates that, the robustness discussed in terms of number of common channels carries little meaning when the sizes of formed clusters are not given consideration.
Besides, cluster size plays import roles in certain aspects.
For instance, cluster size is one decisive factor in power preservation~\cite{clustering_globecom11, EnergyEfficientClusteringRouting_2015}, and it also influences the accuracy of cooperative spectrum sensing~\cite{Consensus_based_clustering12}.
Hence, cluster size should be given consideration when discussing cluster robustness against primary users.

In this paper, a decentralized clustering approach ROSS (RObust Spectrum Sharing) is proposed to cover the issues of robustness and size control of clusters in CRN.
ROSS is able to form clusters with desired sizes, and the generated clusters are more robust than other clustering scheme which has claims on cluster robustness, \ie more secondary users residing in clusters against increasing influence from primary users.
Compared with previous work, ROSS involves much less control messages, and the generated clusters are significantly more robust.
%ROSS selects cluster head through coordination within its neighborhood, and then cluster membership is decided locally and its fast convergence is proved under game theoretic framework. 
%For our scheme we can prove convergence in cluster formation phase and resolve ambiguities with respect to cluster membership in a game-theoretic setting. 
We also propose the light weighted versions of ROSS, which involve less overheads and thus are more suitable for mobile networks.
Throughout this paper, we refer the clustering schemes on the basis of ROSS as \textit{variants of ROSS}, \ie the fast versions, or that with size control feature.

The rest of paper is organized as follows. 
After reviewing related work in section~\ref{related_work}, we present our system model in Section~\ref{sec:model}. 
Then we introduce our clustering scheme ROSS and its variants in section~\ref{ross}.
The clustering problem is given through analysis and a centralized scheme is proposed in section~\ref{centralized_scheme}.
Extensive performance evaluation is in section~\ref{performance}.
Finally, we conclude our work and point out direction future research in section~\ref{conclusion}.


\section{Related Work}
\label{related_work}

Prior to the emergence of open spectrum access, as an important method to manage network, clustering has been proposed in for ad hoc networks~\cite{Kawadia03,Lin97adaptiveclustering,Basagni99}, wireless mesh networks and sensor networks~\cite{Abbasi_survey_07}. 
In ad hoc and mesh networks, the major focus of clustering is to preserve connectivity (under static channel conditions) or to improve routing.
In sensor networks, the emphasis of clustering has been on longevity and coverage.
Overhead generated by clustering in ad hoc network is analysed in~\cite{clusterRoutingOverhead02infocom, clusterRoutingOverhead_wcnc04}.



As to cognitive radio networks, clustering schemes are also proposed, which target different aspects.
Work~\cite{Consensus_based_clustering12} improves spectrum sensing ability by grouping the CR users
with potentially best detection performance into the same cluster.
Clustering scheme~\cite{clustering_globecom11} obtains the best cluster size which minimizes power consumption caused by communication within and among clusters.
\cite{clustering_globecom11} proposes clustering strategy in cognitive radio network, which looks into the relationship between cluster size and power consumption and accordingly controlling the cluster size to decrease power consumption.
%The works~\cite{} introduced in Introduction section don't provide solution that how are the clusters formed.
%There have been several clustering schemes tailored for CRNs. 
Cogmesh is proposed in~\cite{Chen07} to construct clusters by the neighbour nodes which share local common channels, and by interacting with neighbour clusters, a mesh network in the context of open spectrum sharing is formed.
Robustness issue is not considered by this clustering approach.
\cite{TWC2012_cooperative_communication} targets on the QoS poisoning and energy efficiency. 
This approach first decides on the relay nodes which minimize transmission power consumption, then the chosen nodes become cluster heads and clusters are formed in a dynamic coalition process.
This work emphasis on power efficiency and doesn't take into account the channel availability and the issue of robustness of the formed clusters.
In~\cite{Zhao07, Affinity_clustering_09icccn}, the channel available to the largest set of one-hop neighbours is selected as common channel which yields a partition of the CRN into clusters. 
This approach minimizes the set of distinct frequency bands (and hence, the set of clusters) used as common channels within the CRN.
However, bigger cluster sizes generally lead to less options within one cluster to switch to if the common channel is reclaimed by a primary node. 
Hence, this scheme does not provide robustness to formed clusters. 
\cite{cluster_EW10} deploys cluster structure in order to implement common channel control, medium access  with multiple channel and channel allocation. 
The node with the maximum number of common channels within its k-hop neighborhood is chosen as cluster head, but how to avoid one node appearing in multiple clusters is not given consideration.

Clustering robustness is considered in~\cite{Lazos09, LIU_TMC11_2}.
The authors propose a distributed scheme where the metric is the product of cluster size and the number of common control channels.
This scheme involves both cluster size and number of CCCs, but it is inherently flawed.
With the metric, cluster could be formed only due to one factor of the two, e.g. a spectrum rich node will exclude its neighbour to form a cluster by itself.
%emphasis on improving the numbers of common channels within clusters, in order to strengthening robustness of clusters, but the perused metric is not examined or proved to be able to maintain cluster structure under vigorous intensity of primary users’ activities.
%The authors consider the balance between the number of idle common channels within cluster and cluster size and propose an algorithm that increases the number of common channels within clusters. 
%However, this work neglects the issue of connectivity between clusters. 
%One drawback of this scheme is, in order to increase the number of common channels within clusters, the scheme excludes certain CR nodes from the formed clusters, so that isolated nodes have to form clusters themselves. 
Besides, this scheme leads to a high variance on the size of clusters, which is not desired in certain applications as discussed in~\cite{clustering_globecom11, cluster_EW10}.



\section{System Model}
\label{sec:model}
%Let us consider a two dimensional area where primary and secondary users coexist together.
%They share the licensed channels according to the spectrum overlay model, where secondary transmitters are only allowed to transmit when the prifcan exchange its spectrum sensing resulmary users are detected as being idle, and they should vacate the working channel when the presence of primary user is detected.
%The set of primary users and secondary users are presented by $\mathcal{P}$ and $\mathcal{N}$ separately, there are $|\mathcal{P}| = P$ and $|\mathcal{N}| = N$.
%Cognitive radio ad hoc network is constituted by all the secondary users in $\mathcal{N}$.
%The collection of non-overlapping licensed frequency bands is denoted as $\mathcal{F}$ with $|\mathcal{F}| =F$, which is shared by the the primary and secondary users.
%We assume that primary users have a relatively low variation in activity (periods of activity and inactivity in the range of seconds or minutes).
%%
%Primary users access the allocated channels in $\mathcal{F}$ according to its need without sending any explicit notification to secondary users.

We consider a set of cognitive radio users $\mathcal{N}$ and a set of primary users distributed on a two-dimensional Euclidean plane.
%We denote the set of these nodes as $V$ which is partitioned into sets of primary users $V_P$ and secondary users $V_N$, i.e., $V~=~V_P~\dot\cup~V_N$. 
These users share a number of non-overlapping licensed channels according to the spectrum overlay model. 
The set of these licensed channels is denoted as $\mathcal{K}$. 
As secondary users, the CR users are allowed to transmit on a channel $k \in \mathcal{K}$ only if no primary user is detected being accessing channel $k$. 
Further, we consider a \textit{cognitive radio ad-hoc network} which consists of all secondary users and does not contain any primary user.

%The available channels sensed on secondary user $i$ is denoted by $V_i$ and there is $\vert V_i \vert \leq F$. % indicating the total number of 
%%Each secondary user notifies its neighbours of the sensing result about the available channels.

Secondary users conduct spectrum sensing independently and sequentially on all licensed channels.
The sensing duration and frequency on one channel is a research topic~\cite{sensing_survey_2009}, and we assume that every node can detect the presence of primary user on each channel with certain accuracy. \footnote{The spectrum availability can be validated with a certain probability of detection. Spectrum sensing/validation is out of the scope of this paper.}
%In our system model, we assume that a secondary node $n$ is able to detect for each channel $c$ whether any primary user utilizes $c$.
We denote $K_i \subseteq \mathcal{K}$ as the set of available channels for $i$.

%One dedicated control channel is assumed to be available for all the secondary nodes to exchange control messages with neighbours in the process of cluster formation.
%Note that the assumption of dedicated control channel is to simplify the discussion so that we can focus on the kernel of this paper, the robustness of clusters.
%Actually, the control messages involved in the clustering process can be conveyed on available licensed channels through rendezvous process by channel hopping~\cite{channelHopping_Rendezvous_2014, Gu_distributed_rendezvous_2014}. 
%Over the control channel, secondary users exchange their spectrum sensing results $V_{i}$ with one hop neighbours. 
%Secondary users have the same transmission range on both licensed and control channel, and the communication links are reciprocal.

We adopt the unit disk model~\cite{unitDiskModel} for the transmission of both primary and CR users.
Both primary users and CR users have fixed transmission ranges respectively, and the all the channels are regarded to be identical in terms of signal propagation.
If a CR node locates within the transmission range of primary user $p$, that CR node is not allowed to use the channel $k(p)$.

We assume that in addition to the licensed channels, there is one dedicated control channel.
This control channel could be in ISM band or other reserved spectrum which is exclusively used for transmitting control messages.
Actually, the control messages involved in the clustering process can be transmitted on available licensed channels through a rendezvous process by channel hopping~\cite{channelHopping_Rendezvous_2014, Gu_distributed_rendezvous_2014}, i.e., two neighbouring nodes establish communication on the same channel.
Over the control channel, a secondary user $i$ can exchange its spectrum sensing result $K_i$ to any $i' \in \text{Nb}(i)$.
It is available for any secondary node $i$ to exchange control messages with any other node in its proximity (or neighborhood) $\text{Nb}(i)$ during the cluster formation phase.
$\text{Nb}(i)$ is simply defined as the set of nodes located within the transmission range of $i$. 





%Primary users also have fixed transmission range on the licensed channels\footnote{This assumption is made to simplify the discussion, and doesn't affect the effectiveness of the proposed scheme}.

%As to a pair of secondary users, when the Euclidean distance between them is smaller than secondary users' transmission range, we assume the pair is able to communicate on both control channel and licensed channel, and the both are considered to be neighbours of each other. DEFINE DISTANCE! \textcolor{brown}{distance, done}
%
If a secondary user $i$ is not in the transmission range of a primary user $p$, $i$ can certainly not detect the presence of $p$.
As the transmission range of primary users is limited and secondary users are located at different locations, different secondary users may have different views of the spectrum availability, i.e., for any $i, i' \in \mathcal{N}$, $K_i = K_{i'}$ does not necessarily hold.	
%
%Neighbourhood establishment and maintenance with control channel and according to a neighborhood discovery protocol which is out of scope of our work.
%While primary users are assumed to be fixed, secondary users can be mobile.
%Validation refers to the process of ensuring that no primary transmission is actually taking place on the respective channel.
%All $N$ secondary users constitute an ad-hoc network in which data can be transmitted from one certain node to any other node should available c.hannels are available on the source node, destination node and other intermediate nodes.
%Communication on licensed channels is possible only when nodes $i, j$ are both located in each other's transmission range and both share a validated licensed channel.
As the assumed $0/1$ state of connectivity is solely based on the Euclidean distance between secondary users,
%CRN can be represented by a graph $\mathcal{G}(I,\mathcal{E})$, where $\mathcal{E}=\lbrace(i,j,v) \vert i, j \in \mathcal{N} \wedge v\in V_i \wedge v\in V_j \rbrace$ is wireless link between any secondary node $i$ and its neighbour $j$ with licensed channel $v$.
%Due to relatively low primary user dynamics, time index is omitted here.
%%In order to perform clustering, CR users first need to establish their neighborhood.
%For secondary node $i$, its neighborhood $Nb_i$ consists of all the secondary users locating within its transmission range, regardless whether common licensed channels exist or not. 
%% and have at least one common channel with node $i$ each, i.e. $ j\in Nb_i \Rightarrow V_i\cap V_j\neq \emptyset$.
%%The clustering phase is initialized during which any control message is again conveyed by the control channel.
%In the rest of this paper, \textit{channel} only refers to the licensed common control channel unless the dedicated control channel is particularly mentioned.

A cognitive radio network can be represented as an undirected graph $G = (\mathcal{N}, E)$, where $E \subseteq \mathcal{N} \times \mathcal{N}$ such that $\{i, i'\} \in E$  if, and only if, there exists a channel $k \in \mathcal{K}$ with $k \in K_{i} \cap K_{i'}$. Note that we consider the channel availability only for \textit{one} snapshot of time. For the rest of this paper the word channel is referred to licensed channel, if the control channel is not explicitly mentioned.



\subsection{Clustering}
\label{def_cluster}
%We give the description of cluster in the context of CRN, all the clusters discussed in this paper comply with it.
%A cluster $C$ is composed with one cluster head and cluster members, which satisfies the following conditions:
%\begin{itemize}
%\item Cluster head $H_C$ is able to communicate with any cluster member \textit{directly}, \ie for any cluster member $i\in C$, there is $i\in \texttt{Nb}_{H_C}$.
%\item There exists as least one common control channel in the cluster, \ie $\cap_{i\in C} V_i \neq \emptyset$.
%\end{itemize}
In this section, we describe what a cluster in the context of CRNs means.
A cluster $C \subseteq \mathcal{N}$ is a set of secondary nodes consisting of a cluster head $h_{\textsf{C}}$ and a number of cluster members.
The cluster head is able to communicate with any cluster member directly.
In other terms, for any cluster member $i \in C$, $i \in \text{Nb} (h_C) $ holds.

%A cluster head coordinates the activities of cluster members, \ie it notifies all the members to vacate a channel if a primary user's presence is detected by one cluster member. Further it also notifies the members to use a different common control channel for payload communication. %(\textcolor{red}{(Not clear what the meaning of payload communication is. Why not omitting payload here?)} \textcolor{brown}{payload communication happens when the clusters are formed, and start to support network functionalities, e.g. routing. Thus I think it is necessary to leave it here.}


----------- No modifications from here on ... -------------

Cluster is denoted as $C(i)$ when its cluster head is $i$.
Cluster size of $C(i)$ is written as $|C(i)|$.
%, for the sake of concision, $C_i$ is also named as cluster $i$ in the following part of this paper.
$ K(C) = \cap_{i\in C} K_i$, $K(C)$ denotes the set of common control channels in cluster $C$.
Clustering is performed periodicity, as secondary users are mobile and the channel availability on secondary users are changing as primary users change their operation state.%, or the most current clusters can not maintain due to lack of CCCs when primary users' operation is intense.





%We propose distributed scheme ROSS to form clusters to generate robust clusters, and in the same time clusters have preferred sizes, \ie fewer number of singleton clusters (the cluster which consist with only one CR node) compared with state of art.

%Besides, cluster size is also considered in to clustering solution.
%Size preference can be met after minor modifications on ROSS.


%The metric is summation of the number of channels available to be used for each node when they reside in a certain cluster, together with a cost for not following the desired cluster size, \ie when the desired cluster size is $\delta$ and the other cluster sizes are denoted as $\delta'$, the metric is, 
%\begin{equation}
%\label{metric}
%\sum_{i=1}^{N}(ICC_i)
%\end{equation}
%$N$ is the number of CR nodes in the network.


%Also, we refer to the common channels between neighbouring clusters by the term \textit{outward common channels} (OCC). We define the set of OCCs of cluster $C$ to be the set of available common channels between any member of $C$ and any other CR user of a neighbouring cluster:
%\begin{equation}
%\label{numocc}
%R_{C}=\bigcup_{j\in C, k\in N_j, k\notin C}(V_{j}\cap V_{k})
%\end{equation}

%In the process of clustering, when there is no restriction on cluster size, the metric will only be the summation of the number of channels for each node when they reside in a certain cluster.
%When we try to maximize the summation of ICCs, an obvious correlation between cluster size and number of ICCs is encountered, \ie when each node constitutes one cluster, the aforementioned metric will be maximized.
%The goal of the proposed clustering scheme is to let as more CR nodes as possible to form clusters, meanwhile the clusters have more common channels and preferred cluster size.


%\textcolor{red}{The notation is not well chosen. Does it make sense to have only one letter for a potential cluster? What happens if we have two clusters? Find some comments next to your proposed notation.}


\begin{table}[h!]
\caption{Notations in robust clustering problem}
\label{tab1}
\centering
\begin{tabular}{llr}
\toprule
%\multicolumn{2}{c}{Item} \\
%\cmidrule(r){1-2}
Symbol & Description \\
\midrule
$\mathcal{N}$  & collection of secondary users, $N=|\mathcal{N}|$\\
$\mathcal{K}$	& set of licensed channels\\
$k(i)$ & the working channel of user $i$\\
$\text{Nb}(i)$ & the neighborhood of CR node $i$    \\
$C(i)$ & a cluster whose cluster head is $i$  \\
%$k_i$   & set of available channels at CR node $i$  \\
$K_i$   & the set of available channels at CR node $i$  \\
$K(C(i))$   & the set of available CCCs of cluster $C(i)$ \\
%$h(C)$ & cluster head of a cluster $C$\\
$h_C$ & the cluster head of a cluster $C$\\
%$\text{CH}$ & cluster head\\
$\delta$ & desired cluster size\\
$S_i$ & a set of claiming clusters, each of which includes \\
& debatable node $i$ after phase I\\
$d_i$  & individual connectivity degree of CR node $i$\\
$g_i$  & social connectivity degree of CR node $i$\\
$C_i$  & the $i$th legitimate cluster (only appear in Sec.~\ref{centralized_opt})\\
\bottomrule
\end{tabular}
\end{table}




\section{Distributed Coordination Framework: Clustering Algorithm}
\label{ross}

In this section, we introduce the distributed clustering scheme ROSS which leads to robust clusters against primary users' influence.
ROSS consists of two cascaded phases: \textit{cluster formation} and \textit{membership clarification}.
With ROSS, CR nodes form clusters on the basis of the proximity of the available spectrum in their neighbourhood. Afterwards, CR nodes belong to one certain cluster. %by interacting with certain neighbours.
%is based on the spectrum sensing results in its neighbours , then
	

\subsection{Phase I - Cluster Formation}
\label{phaseI}
After conducting spectrum sensing and communication with neighbours, every CR node is aware of the available channels available for themselves as well as for all its neighbors.
For each CR user $i$, two metrics are proposed to characterize the spectrum proximity between $u$ and its neighborhood:
%\newtheorem{def1}{Definition}
%\label{def1}
%\begin{def1}
%Connection vector $\left\{d_i,g_i\right\}$ of CR node $i$:
%spectrum connectivity degree: 
%\begin{equation}
%\label{d_i}
%d_i=\sum_{j\in N_i}\vert V_i\cap V_j\vert
%\end{equation}
%Social Connection degree:
%\begin{equation}
%\label{g_i}
%g_i=|\bigcap_{j\in N_i}V_j|
%\end{equation}
%\end{def1}
\begin{itemize}

\item \textit{Individual connectivity degree} $d_i$: $d_i=\sum_{j\in \text{Nb}(i)}\vert K_i\cap K_j\vert$. It denotes the sum of the numbers of common controls channels between node $i$ and every neighbour.
It is an indicator of node $i$'s adhesive property to the CRN. 

\item \textit{Social connectivity degree} $g_i$: $g_i=|\bigcap_{j\in \text{Nb}(i)\cup i}K_j|$. It is the number of common channels available to$i$ and all its neighbors.
$g_i$ represents the ability of $i$ to form a robust cluster with its neighbours.
\end{itemize}
Individual connectivity degree $d_i$ and social connectivity degree $g_i$ together form the \textit{connectivity vector}.
Figure~\ref{fig1} illustrates an example CRN where each node's connectivity vector is calculated and shown.	
\begin{figure}[ht!]
  \centering
\includegraphics[width=0.7\linewidth]{figure1.pdf}
% \includegraphics{figure1.pdf}
	\caption{Connectivity graph and the connectivity vector $(d_i, g_i)$ at each node. The available channels sensed by each CR node are: $K_A=\{1,2,3,4,5,6,10\}, K_B=\{1,2,3,5,7\}, K_C=\{1,3,4,10\}, K_D=\{1,2,3,5\}, K_E=\{2,3,5,7\}, K_F=\{2,4,5,6,7\}, K_G=\{1,2,3,4,8\}, K_H=\{1,2,5,8\}$. The dashed edge indicates the end nodes are within each other's transmission range. The number of common channels between the two nodes is shown by the edge.}
	\label{fig1}
\end{figure}

After introducing the connectivity vector, we proceed to introduce the first phase of algorithm ROSS.
To put it briefly, in this phase cluster heads are determined in the beginning. Then clusters are formed on the basis of the cluster heads' neighborhoods.


\subsubsection{Determining Cluster Heads and Form the Initial Clusters}
In this phase, each CR node decides whether it is a cluster head by comparing its connectivity vector with its neighbors.
When CR node $i$ has lower individual connectivity degree than any neighbors except for those which have already become cluster heads (the appearance of cluster heads will be explained in Section~...), then node $i$ becomes clusters head.
%In other words, CR node $i$ becomes cluster head if $d_i > d_k, \forall k\in \texttt{Nb}_i\setminus CHs$ ($CHs$ donate the cluster heads existing in $\texttt{Nb}_i$).
If there is another CR node $j$ in its neighborhood which has the same individual connectivity degree as $i$, \ie $d_j = d_i$ and $d_j < d_{k}, \forall k\in \text{Nb}(j)\setminus \{\text{CHs}\cup i\}$, then the node out of $\{i, j\}$ with higher social connectivity degree becomes cluster head. The other nodes become a member of that cluster. 
If $g_i = g_j$ as well, the node ID is used to break the tie, \ie the one with smaller node ID becomes the cluster's head.
%
The node which becomes cluster head broadcasts a message of its eligibility of being cluster head to notify its neighbours, and claims its neighbourhood as its cluster.
The pseudo code for the cluster head decision and the initial cluster formation is shown in Algorithm~\ref{alg0} in the appendix.

After receiving the notification from a cluster head, a CR node $i$ is aware that it becomes a member of a cluster. Consequently, $i$ sets its individual connectivity degree to a positive number $M > |\mathcal{K}| \cdot N$. %, which is smaller than all the possible individual connectivity degrees in the CR network.
Then $i$ broadcasts its new individual connectivity degree to all its neighbors. 
%wrong:
%Note that there is no cluster head which receives the notification on cluster head eligibility, and there is a Lemma on this.
%\begin{theorem}{lemma}
%There is no cluster head receiving the notification message on cluster head eligibility, which is sent from a node which newly becomes a cluster head.
%\end{theorem}
%\begin{proof}
%We use contradiction to prove. 
%Assume a CR node $j$ is cluster head, and receives a notification on cluster head eligibility form CR node $i$. 
%According to the assumption on the reciprocal communication link, $i$ should have received the notification message from $j$ when $j$ becomes cluster head.
%Then CR node $i$ will set its individual connectivity degree to zero, and has no possibility to become a cluster head.
%\end{proof}
We manipulate the individual connectivity degree of the CR nodes which are included in certain clusters. Hence, nodes located outside of the a formed cluster can possibly become cluster heads or can also be included into other clusters.
When a CR node $i$ is associated to multiple clusters, \ie $i$ has received multiple notifications of cluster head eligibility from different CR nodes, $d_i$ is still set to $M$.
%Then every CR node becomes either cluster head or one member of at least one cluster.
%In other words, with this manipulation on its individual connectivity degree, every CR node in the network will be involved in the clustering process, the proof will be given later.
%Every CR user is involved in the clustering process and becomes either cluster head or member is described in theorem~\ref{clustering:theorem}.
We have the following theorem to show that as long as a secondary user's individual connectivity degree is greater than zero, that secondary user will eventually be integrated into a certain cluster, or it eventually becomes a cluster head.
\begin{theorem}
\label{clustering:theorem}
Given a CRN, every secondary user is included into at least one cluster within $N$ steps.
\end{theorem}
Here, \textit{Step} means one secondary user conducts Algorithm~\ref{clustering:theorem} once.
The Proof is in Appendix~\ref{proof_clustering:theorem}.
According to Theorem~\ref{clustering:theorem}, we can assign reasonable amount of time for phase I to complete.

%This judgement is conducted periodically, and phase I ends after every node ascertains it is cluster head or not.
Let us apply Algorithm~\ref{alg0} to the example shown in Figure~\ref{fig1}.
Node $B$ and $H$ have the same individual connectivity degree, $d_B=d_H$, but as $g_H=2>g_B=1$, node $H$ becomes the cluster head and cluster $C_H$ is $\{H, B, A, G\}$.
	




\subsubsection{Guarantee the Availability of Common Control Channel}
%After deciding itself being cluster head, CR node broadcasts to notify its neighbours on the dedicated control channel, meanwhile, $i$'s initial cluster is formed immediately, which is $i$'s neighbourhood except for those nodes which have become cluster heads, \ie $C_i=(\text{Nb}_i\setminus \text{CHs})\cup i$.
After executing phase I of ROSS, there are some secondary users which become cluster heads.
The cluster head and its neighbourhood (except for the CHs) become a cluster.
It is possible that there is no CCCs for some formed clusters, and we solve this problem with the following method.

As decreasing cluster size increases the number of CCCs within the cluster, to have at least one CCC, certain nodes are eliminated.
The sequence of elimination is performed according to an ascending list of nodes which are sorted by the number of common channels between the nodes and the cluster head. 
In other words, the cluster member which has the least number of common channels with the cluster head is excluded first.
If there are multiple nodes having the same number of common channels with the cluster head, the node whose elimination brings in more common channels will be excluded.
If this criterion meets a tie, the tie will be broken by deleting the node with smaller node ID.
It is possible that the cluster head excludes all its neighbours and resulting in a \textit{singleton cluster} which is composed by itself.
%At the end of this procedure every formed cluster has at least one common channel.
The pseudo code for cluster head to obtain at least one common channel is shown in Algorithm~\ref{alg_size_control_available_CCC}.
As to the nodes eliminated in this procedure, they restore their original individual connectivity degrees, and become either cluster heads or get included into other clusters afterwards according to Theorem~\ref{clustering:theorem}.




\subsubsection{Cluster Size Control in Dense CRN}
\label{cluster_pruning}

In the introduction section, we have stated that cluster size should be given consideration to justify the concept of robustness of clusters, \ie without specifying requirement on cluster sizes, small clusters will be generated to obtain more CCCs.
Except for cooperative sensing, clusters need to conduct some other functionalities.
When cluster size is large, there will be substantial burden on cluster heads to manage the cluster members, which is a challenge for resource limited cluster heads, thus the cluster size should fall in a desired range~\cite{Chen04clusteringalgorithms, capacity_cluster_06}.

In the following we illustrate the pressing necessity to control the cluster size when CRN becomes dense via both theoretical analysis and simulation.
Assuming the secondary and primary users are evenly distributed and primary users occupy the licensed channels randomly, then both CR nodes density and channel availability in the CRN can be seen to be spatially homogeneous.
%Based on Algorithm~\ref{alg0}, cluster heads are the CR nodes which possess the biggest individual connectivity degrees in their neighborhood respectively, and they are surrounded by CR nodes.
%In contrast, CR nodes residing on edge are unlikely to become cluster heads as their neighbourhoods are only half the nodes locating away from edge.
The formed clusters are the neighbourhoods of cluster heads, and the neighbourhood is decided by the transmission range and network density.
%
We consider a cluster $C(i)$ where $i$ is CH in a dense CRN. 
When we don't consider the CHs which could appear within $i$'s neighbourhood in the procedure of guaranteeing CCCs, according to Algorithm~\ref{alg0}, the nearest cluster heads could locate just outside node $i$'s transmission range.
An instance of this situation is shown in Figure~\ref{clusters_denseNetwork}.
%
In the figure, black dots represent cluster heads, the circles denotes the transmission ranges of cluster heads.
Cluster members are not shown in the figure.
%Circles represent the transmission range of cluster head, within which CR nodes are absorbed in cluster.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.4\linewidth]{clusters_denseNetwork_2.pdf}
  \caption{Clusters formation in extremely dense CRN. Black dots are cluster heads, other cluster members are not drawn.}
  \label{clusters_denseNetwork}
\end{figure}
Let $l$ be the length of side of simulation plan square, and $r$ be CR's transmission radius.
Based on the aforementioned analysis and geometry illustration as shown in Figure~\ref{clusters_denseNetwork}, we give an estimate on the maximum number of generated clusters, which is the product of the number of cluster heads in one row and that number in one line, $l/r * l/r = l^2/r^2$.
%Now we verify the estimation with simulation.
%We distribute CR and primary users randomly on a square plain, $r$=10 and $l$=50.
%Network density is increased by adding more CR users.
%We now have a look at how does the network density affect the cluster size when the transmission range is constant.
%This implicates when the cluster size is decided by the density of the network.
%As to SOC, the membership of one cluster is decided after a complex process, and the cluster size is roughly the same with one neighborhood.xxxx
%We can see from the example that although two neighbouring clusters can overlap greatly with each other, no cluster head will be covered by other clusters.
Given $r$=10 and $l$=50, the number of formed clusters is shown in Figure~\ref{number_clusters_scale}.
With the increase of CR users in the network, network density increases linearly (the Y axis label is the average number of neighbours), and the number of formed clusters also increases and approaches to the the upper bound of 25 which complies with the estimation.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{number_clusters_upperBound.pdf}
  \caption{The correlation between the number of formed clusters and network density. Note that the number of neighbours denotes the network density. Simulation is run for 50 times and the confidence interval is 95\%.}
  \label{number_clusters_scale}
\end{figure}

Both the analysis and simulation show that when applying ROSS, after the number of clusters saturates with the increase of network density, the cluster size increases almost linearly with the network density, thus certain measures are needed to curb this problem.
This task falls to the cluster heads.

To control the cluster size, cluster heads prune their cluster members to achieve the desired cluster size.
The desired size $\delta$ is decided based on the capability of the CR users and the tasks to be conveyed.
Given the desired size $\delta$, a cluster head excludes members sequentially according to the following principle, the absence of one cluster member leads to the maximum increase of common channels within the cluster.
This process ends when the size of resultant cluster is $\delta$ and at least one CCC is available.
This procedure is similar with that to guarantee CCCs in cluster, thus the algorithm can reuse Algorithm~\ref{alg_size_control_available_CCC}.

%Note that ROSS generates clusters on basis of cluster heads' neighbourhood, thus the $\delta$ is smaller than the average neighbourhood size.
%As there are extensive overlaps between clusters, the threshold that the cluster size satisfies requirement should be larger than $\delta$.
%In practise, we set this threshold as $5*\delta/2$.
%This process ends when the size of resultant cluster is at most $5*\delta/2$ and at least one CCC is available.




%Figure~\ref{fig2} shows the clusters formed in the example in Figure~\ref{fig1} when the desired cluster size is 3. 

%Especially, it forster the connectivity between the clusters. It can do so, as the nodes with larger connectivity degree are not cluster heads but members. 
%As basin nodes have smaller $d_i$ compared with its cluster members, and many of the cluster members locate between the basin node and neighbor clusters,  %the bigger $d_i$ with bigger robustness of Social Connection, i.e, with bigger $d$ are located around cluster heads, 
%After clusters are formed, with aid of \textit{control channel rotation scheme} proposed in \cite{Lazos09}, intra and inter cluster communication is conducted and for each debable node (XXX Debatable nodes are not defined yet), the membership and channel availablity of the clusters concluding it is known. 


\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.6\linewidth]{figure2.pdf}
  \caption{Clusters formation after the first phase of ROSS. There are some nodes being debatable nodes, \ie belonging to more than one cluster.}
  \label{fig2}
\end{figure}


\subsection{Phase II - Membership Clarification}
%\subsubsection*{Problem Description}
After applying phase I of ROSS to the example in Figure~\ref{fig1}, the resulted clusters are shown in Figure~\ref{fig2}.
We notice nodes $A, B, D$ are included in more than one cluster. 
We refer these nodes as \textit{debatable nodes} as their cluster affiliations are not clear, and the clusters which include the debatable node $i$ are called \textit{claiming clusters} of node $i$, and are denoted as $S_i$.  
Actually, debatable nodes extensively exist in CRN with larger scale.
Debatable nodes should be exclusively associated with only one cluster and be removed from the other claiming clusters, this procedure is called \textit{cluster membership clarification}.
We will introduce the solution for cluster membership clarification in the following.	





%In the second phase of ROSS, debatable nodes will chose only one cluster to reside.

%In particular, the un-affiliation of one debatable nodes from a cluster increasing the set $K_C$ of ICCs of cluster $C$ (at the cost of potentially decreasing $R_C$, the set of OCCs).

%\newtheorem{observation}{Observation}
%\label{observation}
%\begin{observation}
%If the number of nodes within a cluster decreases, the number of common channels will increase or keep constant.
%\end{observation}
%
%\begin{proof}
%Contradiction, To be continued
%\end{proof}
%From observation 1 we know that the procedure of membership clarification will increase the set of common channels for some clusters and accordingly strengthen the robustness of intra connectivity. 

% % % % %	dependancy!
%An debatable node belongs to multiple clusters, and in the same time, it is possible that several debatable CR nodes locate within one same cluster. %Each debatable node tries to increase the sum of ICC of the clusters which it belong to. More specifically, 
%For a debatable node $i\in S_i$ after phase I, as to clarify its membership, it will choose one cluster $C\in S_i$ to stay and withdraw from the other clusters in $S_i$ with the consideration of increasing ICCs within $S_i$ by the largest margin. 

\subsubsection{Distributed Greedy Algorithm (DGA)}
%Debatable node $i$ is aware of all its claiming clusters in $S_i$. 
After Phase I, debatable nodes, \eg $i$ needs to decide one cluster $C\in S_i$ to stay, and thereafter leaves the rest others in $S_i$.
The principle for debatable node $i$ to choose one claiming cluster is that its decision can result in the greatest increase of common channels in all its claiming clusters.
%The set of available channels of one cluster are known by the debatable nodes which locate in that cluster. %this is finished 
Since node $i$ is a neighbour of all the cluster heads in $S_i$, node $i$ is aware of the channel availability on these claiming cluster heads, and the common control channels in these claiming clusters.
With these information, node $i$ is able to calculate how how many more CCCs will be produced in one claiming cluster if $i$ leaves that cluster.
%Based on this calculation, $i$ decides on one claiming cluster to stay and leaves the other claiming clusters.
If there exists one cluster $C\in S_i$, when $i$ leaves this cluster brings the least increased CCCs than leaving any other claiming clusters, then $i$ chooses to stay in cluster $C$.
When there comes a tie, among the relevant claiming clusters, $i$ chooses to stay in the cluster whose cluster head shares the most CCCs with $i$.
In case there are multiple claiming clusters demonstrating the same on the aforementioned criteria, node $i$ chooses to stay in the claiming cluster which has the smallest size.
Node IDs of cluster heads will be used to break tie if the previous rules could not decide on the unique claiming cluster to stay.
The pseudo code of this algorithm is described as Algorithm~\ref{alg4}.
%To conduct Algorithm~\ref{alg4}, debatable node $i$ needs to know the necessary information about its claiming clusters, \ie $K(C)$ (the set of available channels in cluster $C$), $K(h_C)$ (the set of available channels on $C$'s cluster head $h_C$) and $|C|, C\in S_i$ (sizes of $i$'s claiming clusters).
After deciding its membership, debatable node $i$ notifies all its claiming clusters, and retrieves the updated information of the claiming clusters, \eg $K(C)$, $K_{h_C}$, $|C|$, where $C\in S_i$.


This procedure raises a concern that the debatable nodes may never stop changing their affiliations, as debatable nodes' choices seem to be dependent on each other, and the infinite chain effect never ceases.
%  update their choices based on other debatable nodes' choices, and this process 
For example, assuming one debatable node $i$ locates in cluster $C\in S_i$, and $C$ has more than one debatable node except for $i$.
Assuming node $i$ makes decision to stay in the claiming cluster $C$, afterwards one another debatable nodes $j$ decides its affiliation, and there is $j\in C\in S_i$.
When $j$ leaves cluster $C$, which decrease cluster $C$'s cluster size, and could possibly trigger node $i$ to alter its previous decision to leave $C$, as $C$'s size is smaller now and leaving it may result in more increase of CCCs in $S_i$.
At this point of time, debatable node $j$ may rejoin cluster $C$ duo to the changes in $S_j$, then both node $i$ and $j$ are facing the \textit{same}\footnote{Actually it is not totally same as before, as there are some new changes within $S_j$.} situation again.
%To maximize the increase of common channel in clusters in $S_j$, $j$ either stays in $C$, or leaves cluster $C$ and stay another cluster in $S_j$.
%$j$'s decision changes $C$'s membership, which will affect $i$'s decision.
%Assume $i$ makes decision before $j$ and chooses to stay in $C$, as which brings the most common channels in $i$' claiming clusters, and then it is $j$' turn to choose cluster.
%If $j$ leaves $C$, the smaller cluster $C$ will possibly make $i$ to leave it and join another cluster.
%and the choice of $j$ to stay in $C$ or not possibly changes $C$'s membership and  which potentially further triggers node $i$ to alter its previous decision. 
Thence, we must answer this concern raised when implementing ROSS-DGA.
%, and if it converges, how good such a distributed scheme performs. 
In the following we show that the process of membership clarification can be formulated into a singleton congestion game, and a equilibrium is reached after a finite number of best response updates made by the debatable nodes.

\subsubsection{Bridging ROSS-DGA with Congestion Game}
\label{clustering:phaseII:game}
Game theory is a powerful mathematical tool for studying, modelling and analysing the interactions among individuals.
A game consists of three elements: a set of players, a selfish utility for each player, and a set of feasible strategy space for each player. In a game, the players are rational and intelligent decision makers, which are related with one explicit formalized incentive expression (the utility or cost).
Game theory provides standard procedures to study its equilibriums~\cite{game_for_communication_01}.
In the past few years, game theory has been extensively applied to problems in communication and networking~\cite{Neel06analysisand, Wang_gtc_crn_survey_2010}.
Congestion game is an attractive game model which describes the problem where participants compete for limited resources in a non-cooperative manner, it has good property that Nash equilibrium can be achieved after finite steps of best response dynamic, \ie each player choose strategy to maximizes/minimizes its utility/cost with respect to the other players' strategies.
Congestion game has been used to model certain problems in internet-centric applications or cloud computing, where self-interested clients compete for the centralized resources and meanwhile interact with each other.
For example, server selection is involved in distributed computing platforms~\cite{Cloud_Computing_2010}, or users downloading files from cloud, etc.

To formulate the debatable nodes' membership clarification into the desired congestion game, we observe this process from a different (or opposite) perspective. 
From the new perspective, the debatable nodes are regarded to be isolated and don't belong to any cluster, in other words, their claiming clusters become clusters which are beside them. 
Now for the debatable nodes, the previous problem of deciding which clusters to leave becomes a new problem that which cluster to join.
In the new problem, debatable node $i$ chooses one cluster $C$ out of $S_i$ to join if the decrease of CCCs in cluster $C$ is the smallest in $S_i$, and the decrease of CCCs in cluster $C$ is $\sum_{C\in S_i}\Delta\vert K(C) \vert=\sum_{C\in S_i}({\vert K(C) \vert-\vert K(C\cup i) \vert})$.
The interaction between the debatable nodes and the claiming clusters is shown in Figure~\ref{debatable_nodes_claiming_cluster}.
%The concern on convergence appears again as we have discussed in the previous subsubsection.
We give proof on convergence under game theoretic framework.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.25\linewidth]{singletongame_png.png}
  \caption{Debatable nodes and claiming clusters}
  \label{debatable_nodes_claiming_cluster}
\end{figure}


%In the following we will introduce an \textit{server matching}~\cite{kothari:congestion_serverMatching} problem to illustrate congestion game's application in communication systems.


In the following, we show that the decision of debatable nodes to clarify their membership can be mapped to the behaviour of the players in a \textit{player-specific singleton congestion game} when proper cost function is given.
The game to be constructed is represented with a 4-tuple $\Gamma=(\mathcal{P},\mathcal{R},\sum_{i, i \in \mathcal{P}}, f)$, and the elements in $\Gamma$ are explained below,
%To make the model of this game more clear, we make some change to our original problem. Previously, the nodes in overlapping areas belong to more than one cluster, and our scheme is to remove them out of some clusters to increase the set of common channels within the cluster form which the mode leave. In the new model, we assume all the nodes in overlapping nodes don't belong any cluster and the problem become into how do these nodes decide which cluster to join.

%The components of the game are listed as follows,

\begin{itemize}
%	\item $\mathcal{D}=\left\{1,\ldots,n\right\}$, the set of players (debatable nodes).
	\item $\mathcal{P}$, the set of players in the game, which are the debatable nodes in our problem.
%	\item $\mathcal{R}=\left\{1,\ldots,m\right\}$, the set of resources which player can choose, which are all the clusters in our model.
	\item $\mathcal{R} = \cup S_i, i\in \mathcal{P}$, denotes the set of resources for players to choose, in our problem, $S_i$ is the set of claiming clusters of node $i$, and $\mathcal{R}$ is the set of all claiming clusters.
	\item Strategy space $\sum_i, i \in \mathcal{P}$ is the set of claiming clusters $S_i$.
	As debatable node $i$ is supposed to choose only one claiming cluster, then only one piece of resource will be allocated to $i$.%, accordingly this congestion game is a singleton game.
	%when $i$ makes decision, only one resource (one claiming cluster) from the allowed resources is allocated.
	
	%\item We denote by $\mathcal{S}=\left(\mathcal{S}_1,\ldots,\mathcal{S}_n\right)\in \sum_1\times \cdots\times\sum_n$ the state of game where player $i$ plays strategy $\mathcal{S}_i\in \Sigma_i$.
	
	\item %For the clusters which are possible destination of debatable nodes, the decrease of common channels caused by different debatable node' join can be different because of the heterogeneity of channel availability within itself and on the debatable nodes. %furthermore, the sequence of debatable node's join can also alter the decrease. 
	The utility (cost) function $f(C)$ as to a resource $C$. 
	$f(C) = \Delta\vert K^i(C)|, C\in S_i$, which represents the decrease of CCCs in cluster $C$ when debatable node $i$ joins $C$.
	As to cluster $C\in S_i$, the decrease of CCCs caused by the enrolment of debatable nodes is $\sum_{i:C\in S_i, i\rightarrow C} \Delta\vert K^i(C) \vert$. 
$i\rightarrow C$ means $i$ joins cluster $C$.
Obviously this function is non-decreasing with respect to the number of nodes joining cluster $C$.
	
The utility function $f$ is not purely decided by the number of players accessing the resource (debatable nodes join claiming clusters), which happens in a canonical congestion game.
The reason is in this game the channel availability on debatable nodes is different.
Given two same groups of debatable nodes and their sizes are the same, when the nodes are not completely the same (neither are the channel availabilities on these nodes), the cost happened on one claiming cluster could be different if the two groups of debatable nodes join that cluster respectively.
%In a canonical congestion game, the cost (or pay off) is function of only the number of players occupying the resource, and is mono-
%In this new game, the cost function 
Hence, this congestion game is player specific~\cite{Ackermann06purenash}.
In this game, every player greedily updates its strategy (choosing one claiming cluster to join) if joining a different claiming cluster minimizes the decrease of CCCs $\sum_{i:C\in S_i} \Delta\vert K^i(C) \vert$, and a player's strategy in the game is exactly the same with the behaviour of a debatable node in the membership clarification phas.


%	\item The Rosenthal's potential function \cite{Rosenthal} of this congestion game is given by:
%	\begin{equation*}
%	\phi(S)=\sum_{C\in\mathcal{R}} \sum_{i:C\in S_i} \Delta\vert K^i_C \vert   	
%   	%\sum_{i=1}^N \Delta^{i}_{p}(S)=\sum_{i=1}^N \sum_{r\in S_i}\Delta^{i}_{r}(t)	
%  	% \Delta =\sum_{i=1}^N w_i (x_i - \bar{x})^2 .
%	\end{equation*}
%All the players in this game greedily update their strategy to minimize the potential function (congestion), this process is exactly the same with the network behaviour under \textit{Distributed Greedy Algorithm}. 

%	\item It is an asymmetric game because the sets of strategies shared by different players are different.
%	\item The total cost is: 
%\begin{equation*}
%   \sum_{i=1}^N \Delta^{i}_{p}(S)=\sum_{i=1}^N \sum_{p\in s_i} \Delta^{i}_{p}(n_p(S))
%  % \Delta =\sum_{i=1}^N w_i (x_i - \bar{x})^2 .
%\end{equation*}

%This is the global objective we want to minimize.
\end{itemize}

%Singleton congestion game is a special type of matroid game~\cite{Milchtaich1996111,}. 
%It is known that player-specific matroid congestion game admit pure equilibrium, 

As to singleton congestion game, there exists a pure equilibria which can be reached with the best response update, and the upper bound of number of steps before convergence is $n^2*m$~\cite{Ackermann06purenash}, where $n$ is the number of players, and $m$ is the number of resources.
In our problem, the players are the debatable nodes, and the resources are the claiming clusters.
Thus the upper bound of the number of steps can be expressed as $\mathcal{O}(N^3)$.

In fact, the number of steps which are actually involved in this process is much smaller than $N^3$, as both $n$ and $m$ are considerably smaller than $N$.
The percentage of debatable nodes in $\mathcal{N}$ is illustrated in Figure~\ref{percentage_overlapping_node}, which is between 10\% to 60\% of the total number of CR nodes in the network.
The number of clusters heads, as discussed in Section~\ref{phaseI}, is dependent on the network density and the CR node's transmission range.
As shown in Figure~\ref{number_clusters_scale}, the cluster heads take up only 3.4\% to 20\% of the total number of CR nodes.



%and the number of steps towards \textit{Nash Equilibrium} is upper-bounded\footnote{Here we present this with modifying the original conclusion in \cite{Ackermann06purenash} according to our model.} by $ n^2\cdot m $. In our context, $n$ is the number of debatable nodes, $m$ is number of clusters in CRN, %$rk(\Gamma)$ of the matroid  is the cardinality of the maximal independent sets, which is 1 in the case of singleton game, 
%so the total time complexity to achieve the \textit{Nash Equilibrium} using greedy approach is 	.
%%(XXX Just mention after this complexity result the relationship to the system mdeol XX)
%This is upper-bounded (in the worst case) by $O(\vert I\vert^3)$. 
%Based on above model and analysis, phase II converges is Algorithm~\ref{alg4} is run by debatable nodes. 
%Although the game version of DGA can achieve \textit{Nash Equilibrium}, the whole scheme can possibly obtain sub-optimal result.    %, furthermore,this stable state is a local minimum of the global decrease function.
%\todo[inline]{The number of steps, or the upper bound of steps in convergence needs a formal proof}

\subsubsection{Distributed Fast Algorithm (DFA)}
%The convergence speed of DGA is large recalling that the number of steps is of $\mathcal{O}(N^3)$.
We propose a faster version of ROSS, ROSS-DFA, which differs from ROSS-DGA in the second phase.
With ROSS-DFA, debatable nodes decide their respective cluster heads once.
The debatable nodes consider their claiming clusters to include all their debatable nodes, thus the membership of claiming clusters is static and all the debatable nodes can make decision simultaneously without considering the change of membership of their claiming clusters.
As ROSS-DFA is quicker than ROSS-DGA, the former is especially suitable for the CRN where the channel availability changes dynamically and re-clustering is necessary.
To run ROSS-DFA, debatable node executes only one loop in Algorithm~\ref{alg4}.

Now we apply both ROSS-DGA and ROSS-DFA to the toy network in Figure~\ref{fig2} which has been applied the phase I of ROSS.
In the network, node $A$'s claiming clusters are cluster $C(C), C(H)\in S_A$, their members are $\{A,B,C,D\}$ and $\{A,B,H,G\}$ respectively. 
The two possible strategies of node $A$ is illustrated in Figure \ref{fig3}.
In Figure \ref{AinC}, node $A$ staying in $C(C)$ and leaving $C(H)$ brings 2 more CCCs to $S_A$, which is more than that brought by another strategy showed in \ref{AinH}.
After the decisions made similarly by the other debatable nodes $B$ and $D$, the final clusters are formed as shown in Figure~\ref{fig4}.

%Using DFA in phase II, the time complexity is decreased drastically to 1. Thus, the total complexity of ROSS-DFA is $|I|$, while, ROSS-DGA's complexity is $|I|^3$ in the worst case.


\begin{figure}[h]
\centering
\subfigure[Node A stays in cluster $C(C)$, quits $C(H)$, $\Delta\vert K(C(C))\vert+\Delta\vert K(C(H))\vert=2$]{
\includegraphics[width=0.435\linewidth]{figure4AinC.pdf}
\label{AinC}
}
\hspace{.15 in}
\subfigure[Node A stays in cluster $C(H)$, quits $C(C)$, $\Delta\vert K(C(C))\vert+\Delta\vert K(C(H))\vert=1$]{
\includegraphics[width=0.435\linewidth]{figure4AinH.pdf}
\label{AinH}
}
\caption[]{Membership clarification: possible cluster formations caused by node A's different choices} %\subref{node A in $C_C$}, \subref{node A in $C_H$}}
\label{fig3}
\end{figure}

\begin{comment}
As an example, when node A comes to decide which cluster to stay, the memberships of relevant clusters, like $C_C$ and $C_H$, are $\{C,B,D,A\}$ and $\{H,B,G,A\}$ respectively. Before the other two debatable nodes B and D making their belonging clear, cluster $C_C$ and $C_H$ have them in the same time. So node A can decide which cluster to belong to without considering other debatable nodes' action. There are two strategies for node A, which is illustrated in Figure \ref{fig3}. Because staying in cluster $C_C$ brings in more common channels within relevant clusters, node A finally choose cluster $C_C$ to stay and caveat from cluster $C_H$. The membership of $C_H$ is updated in the same time. Node B and D undertake the same process and the clusters are formed finally as Figure \ref{fig4} shows.

Because debatable nodes can conduct membership clarification abased on static membership information of relevant clusters, thus no iteration happens in this process. The time complexity of this algorithm is only decided by the number of debatable nodes, which is maximal $O(\vert I\vert)$. 

As an example, when node A comes to decide which cluster to stay, the memberships of relevant clusters, like $C_C$ and $C_H$, are $\{C,B,D,A\}$ and $\{H,B,G,A\}$ respectively. Before the other two debatable nodes B and D making their belonging clear, cluster $C_C$ and $C_H$ have them in the same time. So node A can decide which cluster to belong to without considering other debatable nodes' action. Figure \ref{AinC}. Because staying in cluster $C_C$ brings in more common channels within relevant clusters, node A finally choose cluster $C_C$ to stay and retreat from cluster $C_H$. The membership of $C_H$ is updated in the same time. Node B and D undertake the same process and the clusters are formed finally as Figure \ref{fig4} shows.
\end{comment}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{final_clustering_ross.pdf}
  \caption{Final formation of clusters, CCCs for each cluster is shown. $K(C(C)),K(C(E)),K(C(H))$ are shown beside corresponding clusters.}
  \label{fig4}
\end{figure}



\section{Centralized Clustering Scheme}
\label{centralized_scheme}

%paper given by james is useful which provides a survey from the perspective of wsn!


%\todo[inline]{check}
The centralized clustering scheme aims to form clusters with desired sizes, meanwhile the total number of common control channels of all clusters is maximized.
In the following, we refer this problem as \textit{centralized clustering}, and give the formal problem definition. 


\begin{mydef}
\label{def_centralized_clustering}
\textit{Centralized clustering in CRN.}

Given a cognitive radio network $\mathcal{N}$ where nodes are indexed from 1 to $N$ sequentially.
Based on certain correlation, certain secondary users constitute one cluster $C$.
$1\leq |C| \leqslant k$ where $|C|$ is the size of cluster $C$ and $k$ is a positive integer.
We name the collection of such clusters as $\mathcal{S}=\{C_1, C_2,\ldots,C_{|\mathcal{S}|}\}$, where $\mathcal{S}$ satisfies the following properties: $\bigcup_{1\leq i \leq |\mathcal{S}|} C_i = N$ and $K(C(i))\neq \emptyset$ for any $i$ which satisfies $1\leq i \leq |\mathcal{S}|$.

%, which distinguishes the centralized clustering problem discussed in this paper.
We give a new definition of the number of CCCs, where the number of common control channels is $|K(C)|$ if $|C|>1$, and is zero when $|C|=1$.
We use $f(C)$ to denote the number of CCCs of a cluster $C$ in the new definition.

The centralized clustering problem is to find a subcollection $\mathcal{S}' \subseteq \mathcal{S}$, so that $\bigcup_{C_j\in \mathcal{S}'} C_j = N$, and $C_{j'}\cap C_j =\emptyset$ for $C_{j'}, C_j\in \mathcal{S}'$ and $j'\neq j$, so that $\sum_{C\in \mathcal{S}'} f(C)$ is maximized.
% when $|C|>1$, and $f(\cdot)=0$ when $|C|=1$.
%As to $f(\cdot):z\rightarrow z$, under this problem setting, if only number of common channels are given consideration, only singleton clusters will be preferred, which contradicts to our goal of clustering CR nodes together, thus we choose $f(\cdot)= |K_{C}|\cdot |C|$, the product of number of common channels and cluster size.
The decision version of centralized clustering in CRN is to ask whether there exists a no-empty $\mathcal{S}'\subseteq \mathcal{S}$, so that $\sum_{C\in \mathcal{S}'} f \geqslant \lambda$ where $\lambda$ is a real number.% in stead of to maximize $\sum_{C\in \mathcal{S}'} f$.
\end{mydef}


%The decision version of \textit{weighted exact cover problem}: 
%Given an universe $U$, and collection $S=\{s_1, s_2, \ldots, s_m\}$ where each subset $s_i\subseteq U$ and is given a weight $w_i$, whether there exists a collection of subsets $\mathcal{C}$ and constant number $\lambda$, so that the union of $\mathcal{C}$ equals to $U$, $s_j\cap s_{j'} = \emptyset$ for different $j$ and $j'\in \{1,2,\ldots, m\}$, and $\sum_{i\in J} w_i \geq \lambda$.
\subsection{Complexity of Clustering Problem}
In this section we investigate the complexity of centralized clustering problem.
Theorem~\ref{theorem1} tells centralized clustering problem in CRN is one NP-hard problem.

\begin{theorem}
\label{theorem1}
CRN clustering problem is NP-hard, when the maximum size of clusters $k\geqslant 3$.
%Assume a CRN can be represented by a connected graph, and there is at least one common channel between any pair of neighbours, then forming at least two CR nodes into one cluster is NP-complete.
\end{theorem}
The proof is in Appendix~\ref{proof_theorem1}.



\subsection{Centralized Optimization}
\label{centralized_opt}
%Exact cover problem can be solved with Knuth's Algorithm X~\cite{dancingLinks_Knuth} as it finds out all the instances of exact cover, then we can choose the one with the biggest sum of weights. 
As there is no efficient algorithm to solve clustering problem in CRN, we propose a centralized optimization where the objective function and the constraints are heuristic, then we adopt binary linear programming to solve the problem.
%Note that binary linear programming is in NP-complete.

%This example indicates the chose of $\mathcal{C}$ plays an important role on the resultant clustering strategy.
%Meanwhile, it provide a chance to constrain the cluster size by putting groups with desired sizes into $\mathcal{C}$.

%the maximum size of $S$ is the \textit{Bell number} of $N$, and $S$ contains the conditions cluster .
%In this case, the resultant $\mathcal{C}$ is composed with all the singleton clusters, \ie, the cluster which contains one node, and the objective is -38.

%it is possible that there doesn't exist combination of clusters with the same cluster size.
%We thus list all possible clusters whose sizes are from 2 to one certain number \footnote{this number of decided by the density of CR network, along with the occupation of PUs. We set this number as cluster size of the biggest cluster ever appears when conducting distributed schemes.}, and check each combination of clusters to find the best covering of network on the aspect of number of ICCs per cluster.
%The complexity of computation is thus \bigO$(N^\delta)$, $\delta$ is the preferred cluster size.

%The global optimal clustering scheme with respect to the number of common channels is investigated to show the gap with the distributed schemes.


%We apply this centralized scheme on a network with network size $N$ and cluster size $\delta$.
%There is $N\mod \delta=0$, and the expected number of clusters is $C = N/\delta$.
%These tailored parameters don't harm the validity of the performance gap between the two schemes.

Given a CRN $\mathcal{N}$ and desired cluster size $\delta$, we obtain a collection of clusters $\mathcal{G}$ which contains all the \textit{legitimate} clusters, and the sizes of these clusters are $1,2,\ldots,\delta$.
Legitimate clusters are the clusters which satisfy the conditions in Section~\ref{def_cluster}. 
Note that the legitimate clusters include the singleton ones, so that we can guarantee the partition of any network is always feasible.

With $N=|\mathcal{N}|, G=|\mathcal{G}|$, we construct a constant $G\times N$ matrix $Q_{G\text{x}N}$. 
The element of matrix $Q$ is $q_{ij}$, where the subscript $i$ is the index of legitimate cluster, and $j$ is the node ID of one CR node.
%Note that $C_i$ means $i$th cluster in the collection $\mathcal{G}$, doesn't denote the cluster where cluster head is $i$, this notation is only valid in this subsection.
There are $i\in \{1,2,\cdots,G-1, G\}$, and $j\in \{1,2,\cdots,N-1, N\}$.
Element $q_{ij}= |K(C_i)|$ if node $j\in C_i$, and $q_{ij}= 0$ if $j\notin C_i$.
In other words, each non-zero element $q_{ij}$ denotes the number of CCCs of the cluster $i$ where node $j$ resides.

\begin{figure}[ht!]
\centering
\bordermatrix{~ 		& 1 	& 2 	& 3 	& \cdots & j & \cdots	& N-1 	& N	\cr
                  1 	& |K(C_1)| 	& |K(C_1)| 	& 0 	& \cdots & \cdots &\cdots	& 0 	& 0	\cr
                  2 	& |K(C_2)| 	& 0 	& |K(C_2)| 	& \cdots & \cdots & \cdots 	& 0 	& 0	\cr
				\vdots  	&\vdots & 	 	& 		&  \vdots		& 		& \vdots \cr
				i 	& 0 	& |K(C_i)| 	& 0 	& \cdots  & \cdots & \cdots 	& |K(C_i)| 	& 0	\cr
				\vdots  	&\vdots & 	 	& 		&  \vdots & \cdots & \cdots 		& 		& \vdots \cr
				\vdots 	& \vdots  	& 0 	& 0 	& \cdots & \cdots & \cdots 	& |K(C_i')| 	& 0	\cr
				G  	& |K(C_G)| & \cdots	 	& 		&  \vdots	& \cdots & \cdots& 		& \vdots \cr}	
\caption{An example of Matrix $Q$, its rows correspond to all legitimate clusters, and columns correspond to the CR nodes in the CRN.}
\label{xx}
\end{figure}

We build a $G\times N$ binary variable matrix $X$, which illustrates the clustering solution.
The element of matrix $X$ is binary variable $x_{ij}, i=1, \ldots, G, j=1, \ldots, N$.
%$x_{ij}=1$ denotes node $i$ is in the $j$th legitimate cluster, and the cluster is chosen in the solution, $x_{ij}=0$ means the $j$th legitimate cluster is not adopted by the partition.
%Note that matrix $Q$ contains only constant elements, and matrix $X$ contains only binary variables.
Now, we can formulate the optimization problem as follows,

\begin{equation}
\begin{aligned}
     &\min\limits_{x_{ij}} && \Sigma_{j=1}^N\Sigma_{i=1}^G (-x_{ij}q_{ij} + (1-w_i)*p) \\
     &\text{subject to}   && \Sigma_{i=1}^G x_{ij} = 1, for \forall j=1, \ldots, N \\
   &&& \Sigma_{j=1}^N x_{ij} = |C_i|*(1-w_i), for \forall i=1, \ldots, G \\
   &&& \text{$x_{ij}$ and $w_i$ are binary variables.}\\
   &&& i\in \{1,2, \cdots G\}, \hspace{0.3cm} j\in \{1,2,\cdots N\}
\notag
\end{aligned}
\end{equation}

The objective is a sum of two parts, the first part is the sum of products of cluster size and the corresponding number of CCCs.
The first part is the only metric adopted by the scheme SOC~\cite{Lazos09}.
The second part is the \textit{punishment} for choosing the clusters whose sizes are not $\delta$.
In fact, the second part is particularly designed to eliminates the drawbacks of SOC, \ie SOC produces a large number of singleton clusters and a few very large clusters which access affluent unlicensed spectrum.
In practical computation, we minimize the opposite of the first part, then the punishment is a positive value.

The first constraint restricts each node $j$ to reside in exactly one cluster.
In the second constraint, $w_i$ is an auxiliary binary variable, which denotes whether cluster $C_i$ is chosen by the solution, in particular, 
$$
w_i = \left\{ \begin{array}{rl}
0 &\mbox{if $i$th legitimate cluster $C_i$ is chosen} \\
1 &\mbox{if $i$th legitimate cluster $C_i$ is not chosen} \\
\end{array} \right.
$$
The second constraint regulates that when the $i$th legitimate cluster $C_i$ is chosen, then  there are $|C_i|$ elements in $i$th row in matrix $X$, which are 1.
%
Now we explain how does the mechanism of the punishment in the objective work. 
The parameter $p$ is defined as follows,
$$
p = \left\{ \begin{array}{rl}
0 &\mbox{ if $|C_i|=\delta$} \\
\alpha_1 &\mbox{if $|C_i|=\delta-1$} \\
\alpha_2 &\mbox{if $|C_i|=\delta-2$} \\
\dots
\end{array} \right.
$$
where $\alpha_i>0$ and increases when $|C_i|$ diverges from $\delta$.
Because of $w_i$, any chosen cluster ($w=0$) brings certain \textit{punishment}.
When the chosen cluster's size is desired size $\delta$, the punishment is zero.
In contrary, when the chosen cluster's size diverges from $\delta$, the objective function suffers \textit{loss}.
In particular, when $w_i=0$ and $|C_i|=1$, the punishment is the most severe.
This design doesn't follow the definition of $f(C)$ in Definition~\ref{def_centralized_clustering} strictly, where $f(C_i)=0$ when $|C_i|=1$, but our design echoes the definition by exerting the most severe punishment on the singleton clusters in the clustering solution.
Choice of $\alpha_i$ affects the resultant clusters.



%\begin{figure}[ht!]
%\centering
%\includegraphics[width=0.45\linewidth]{example.JPG}
%\caption{Example of Matrix Q in a 6-node network, cluster size is set as 2}
%\label{xx}
%\end{figure}

%\begin{figure}[ht!]
%  \centering
%  \includegraphics[width=0.5\linewidth]{figure5final.pdf}
%  \caption{Final cluster formation.}
%  \label{fig4}
%\end{figure}



The optimization formulation is an integer linear optimization problem, which is solved by the function $bintprog$ provided in MATLAB.
Note that the proposed centralized solution is heuristic.
We reiterate the reasons for pursuing the heuristic scheme, first, the problem of centralized clustering is NP hard, and there is no efficient solution to solve it.
The second reason is, the collection of legitimate clusters is dependant on the network topology and spectrum availability in the network, thus to each specific CRN, the space of solution is different.


\subsubsection{Example of the Centralized Optimization}
We look into how does the centralized scheme perform in the toy example of the CRN in Figure~\ref{fig1}.
%
We let the desired cluster size $\delta$ be 3.
A collection of clusters $\mathcal{G}$ is obtained, which contains all the clusters satisfying the conditions of cluster in Section~\ref{def_cluster} and the sizes of clusters are 1, 2 or 3. 
$\mathcal{G}=\{\{A\}, \{B\},\dots,\{B,C\},\{B,A\},\{B,H\},\cdots,\{B,A,C\},$\\$\{B,H,C\}, \{A,D,C\},\cdots\}$, $G = |\mathcal{G}|=38$.

When $\alpha_1$ and $\alpha_1$ are set as 0.2 and 0.8, the formed clusters are shown in Figure~\ref{fig:final_clustering_LP}.
%$\{\{D,E,F\},\{A,C,G\},\{H,G\}\}$, the numbers of CCCs are 2, 3, 3.
The resulted clustering solutions from ROSS-DGA/DFA and SOC are shown in Figure~\ref{fig4} and Figure~\ref{fig:final_clustering_soc} respectively.%, clusters are $\{A,B,C,D,G\},\{E,F\},\{H\}$, and the numbers of CCCs are 2, 3, 4.
%The solution resulted from ROSS-DGA is $\{\{B,H,G\},\{C,A\},\{D,E,F\}\}$ (in Figure~\ref{fig4}), the numbers of CCCs are 2, 4, 2.
%ROSS-DFA generates the same result with ROSS-DGA in this example.	

As to the average number of CCCs, the results of ROSS (including both ROSS-DGA and ROSS-DFA), centralized and SOC are 2.66, 2.66, and 3 respectively. 
Note there is one singleton cluster $C(H)$ generated by SOC, which is not preferred.
When we take no account of the singleton clusters, then the average number of common channels of SOC drops to 2.5. 
\begin{figure}[ht]
\begin{center}
\subfigure[Resulted from SOC]{\includegraphics[width=0.435\linewidth]{final_clustering_soc}\label{fig:final_clustering_soc}}
\hspace{0.15 in}
\subfigure[Resulted from the centralized clustering scheme]{\includegraphics[width=0.435\linewidth]{final_clustering_LP}\label{fig:final_clustering_LP}}
\end{center}
\caption{Final clusters formed in the example network when being applied with SOC and the centralized clustering scheme.}
\label{fig:final_clustering}
\end{figure}

 





\section{Performance Evaluation}
\label{performance}
In this section, we evaluate the performances of all the variants of ROSS, \ie ROSS-DGA and ROSS-DFA, and that with cluster size control features.
The latter is referred as ROSS-$\delta$-DGA/ROSS-$\delta$-DFA, where x is the desired cluster size.
We choose SOC as comparison scheme.
To the best of our knowledge, SOC~\cite{Lazos09} is the only work emphasizing on the robustness of clustering structure from all previous work on clustering in CRN. 
The authors of~\cite{Lazos09} compared SOC with other schemes in terms of the average number of CCCs of the formed cluster, on which SOC outperforms other schemes by 50\%-100\%. 
SOC's comparison schemes are designed either for ad hoc network without consideration of channel availability~\cite{Basagni99}, or for CRN but just considering connection among CR nodes~\cite{Zhao07}.
Hence, we only compare the our proposed schemes with SOC to show the ROSS's merits, besides, we compare the variants of ROSS with the centralized scheme to examine the gap with the global optima.
In particular, we investigate the following metrics,

\begin{itemize}
\item \textit{Average number of CCCs per non-singleton cluster.}
This metric shows the robustness of the current non-singleton clusters.
Non-singleton cluster refers the cluster whose cluster size is larger than 1.
SOC~\cite{Lazos09} compares the average number of CCCs per cluster without distinguishing singleton clusters, which is biased as singleton clusters don't contribute to the cluster structure.
%the CR nodes with more channels could be formulated as singleton clusters.
%This happens in SOC solution, whose objective is to improve the average number of common channel over \textit{all} clusters, \ie including the singleton clusters, thus many CR nodes with more channels are turned into clusters.
%As we try to look into the robustness of clusters of CRs, we exclude those singleton clusters.


\item \textit{Number of singleton clusters.}
This is a straight forward metric which reflects the effectiveness of clustering scheme.
The less resulted singleton clusters means more secondary users are benefited from cluster structure.
%robustness of clusters, as this metric directly shows how many nodes can make use of the cluster structure.
When we investigate the performance with moderate and vigorous intensity of primary users' activities, this metric is the antonym of \textit{survival rate}, \ie, the percentage of nodes which are still within certain clusters.
%When we vary the intensity of primary users' activity, \eg from low to medium level by increasing the number of primary users, 



\item \textit{Cluster sizes.}
Specific clusters size is pursued in many applications due to energy preservation and the system design ~\cite{clustering_globecom11}.
We will present the distribution of CRs residing in the formed clusters, and the number of generated clusters through multiple simulations.

\item \textit{Amount of control messages involved.}
We investigate the number of control messages involved in the clustering process.

\end{itemize}

The simulation is conducted with C++. 
Certain number of CRs and PUs are deployed on a two-dimensional Euclidean plane.
%Complying with the system model, the CR node residing within another CR node's transmission range is seen as neighbour of that CR node.
The number of licensed channels is 10, each PU is operating on each channel with probability of 50\%.
All primary and CR users are assumed to be static during the process of clustering.

Simulation is divided into two parts, in the first part, we investigate the performance of centralized scheme, and the gap between the distributed schemes with the centralized scheme.
This part of simulation is conducted in a small network, as there is no polynomial time solution available to solve the centralized problem.
In the second part, we investigate the performance of the proposed distributed schemes thoroughly in the networks with different scales and densities.

\subsection{Centralized Schemes vs. Decentralized Schemes}
10 primary users and 20 CR users are dropped randomly (with uniform distribution) within a square area of size $A^{2}$, where we set the transmission ranges of primary and CR users to $A/3$.
%There are 10 available channels. 
With this setting, the average number of neighbours of one CR user is 4.8.
%Each primary user randomly occupies one channel, and 
CR users are assumed to be able to sense the existence of primary users and identify available channels.
When clustering scheme is executed, around 7 channels are available on each CR node.

The desired cluster size $\delta$ is 3, the parameters used in the \textit{punishment} for choosing the clusters with undesired sizes are set as follows, $\alpha_1 =  0.4$, $\alpha_2 =  0.6$.
Performance results are averaged over 50 randomly generated topologies, and the confidence interval corresponds to 95\% confidence level.

\subsubsection{Number of CCCs in Non-singleton Clusters}
\label{ccc_20}
%We first have a look at the average number of common channels per non-singleton cluster.
Figure~\ref{ccc_per_nonsingleton} shows the average number of common channel of non-singleton clusters,
%as the singleton clusters (in other words unclustered nodes) don't execute any functionalities of clusters, which are has be discussed in Section~\ref{intro}.
from which we can see the centralized schemes outperform distributed counterparts.
As to the distributed schemes, SOC achieves the largest number of CCCs than all the variants of ROSS.
The reason is, SOC is liable to group the neighbouring CRs which share the most abundant spectrum together, no matter how many of them are, thus the number of CCC of the formed clusters is higher.
But this method leaves considerable number of CRs to form singleton clusters.


As to the variants of ROSS, ROSS-DGA with and without size control outperform ROSS-DFA and its size control version respectively, this is due to the procedure that debatable nodes greedily look for better affiliation to improve the number of CCCs.
We also notice that, the size control feature doesn't affect the number of CCCs for both ROSS-DGA and ROSS-DFA.
This is because the desired cluster size happens to be the average size of clusters generated by ROSS-DGA and ROSS-DFA, then the size control functionality doesn't play effect to increase the number of CCCs.



\subsubsection{Number of Singleton Clusters}
%When the number of PUs in CRN increases, or their operation becomes more intensive, some clusters don't seize any CCCs any more, so that the cluster members and the cluster heads become unclustered, or singleton clusters.
%If there is no common control channels available any more because of the new added PRs, the cluster is regarded as destroyed and the former cluster member CRs become unclustered CRs or in other words singleton clusters.
%We investigate the number of singleton clusters with primary users whose intensity of activities are varying.
Initially, clusters are formed with the presence of 10 PUs. 
Afterwards, extra 20 batches of PUs are added to the network sequentially, and each batch includes 5 PUs. 
%The transmission range and channel occupancy of the new PU is the same with the previous ones, \ie transmission range is $A/3$, and one channel out of 10 is randomly chosen to operate.
Figure~\ref{singleton_clusters} shows the number of unclustered CRs with the increasing number of PUs, from which two conclusions are drawn.
%, which indicates the vulnerability of clusters under varying availability of licensed spectrum.
%

\begin{itemize}
\item The centralized scheme with desired size of 2 generates the most robust clusters, and SOC results in the most vulnerable clusters.
The centralized scheme with desired size of 3 doesn't surpass the variants of ROSS.
The reason lies in the the uniformly sized clusters, whereas the variants of ROSS generate considerable amount of smaller clusters which are more likely to survive when PUs' activities become intense.
%The comparison on cluster sizes will be given in Section~\ref{cluster_size}.
% when the number of PRs is 10$\sim$30, when number of PUs is 30$\sim$60, same amount of unclustered CRs are generated with variants of ROSS.
%When there are 75 and more new PRs, centralized scheme with cluster size of 3 results in more unclustered CR nodes than variants of ROSS.

ROSS with size control distinguishes itself as the size control avoid the appearance of the clusters with large size.

\item Greedy algorithm improves survival rate. 
Greedy strategy adopted in the second phase of ROSS improves the robustness of clusters, i.e. ROSS-DGA exceeds ROSS-DFA, and ROSS-$\delta$-DGA surpasses ROSS-$\delta$-DFA.
When the debatable CRs greedily update their affiliation with the claiming clusters, one of the metrics is the maximum increase of CCCs of the demanding clusters. 
This observation complies with the result shown in Figure~\ref{ccc_per_nonsingleton}.

%Meanwhile, sizes of more clusters become smaller also contributes more robustness.

\end{itemize}




%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=0.8\linewidth]{ccc_20.pdf}
%  \caption{Number of common channels for non-singleton clusters, the numbers in the names of schemes annotate the desired cluster size.}
%  \label{ccc_per_nonsingleton}
%\end{figure}
%
%
%\begin{figure}[h]
%  \centering
%  \includegraphics[width=0.8\linewidth]{survival_rate_20.pdf}
%  \caption{Number of CRs which are not included in any clusters}
%  \label{singleton_clusters}
%\end{figure}



\subsubsection{Cluster Size Control}
\label{cluster_size}
%\begin{figure}[h]
%  \centering
%  \includegraphics[width=0.8\linewidth]{cdf_clusterSize_20.pdf}
%  \caption{Cumulative distribution of CRs residing in clusters with different sizes.}
%  \label{size_control}
%\end{figure}


\begin{figure*}[th]
\begin{multicols}{3}
    \includegraphics[width=\linewidth]{ccc_20.pdf}\par\caption{Number of common channels for non-singleton clusters}\label{ccc_per_nonsingleton}
    \includegraphics[width=\linewidth]{survival_rate_20.pdf}\par\caption{Number of CRs which are not included in any clusters}\label{singleton_clusters}
    \includegraphics[width=\linewidth]{cdf_clusterSize_20.pdf}\par\caption{Cumulative distribution of CRs residing in clusters with different sizes}\label{size_control}
\end{multicols}
\caption{Comparison between the distributed and centralized clustering schemes in a small network ($N$ = 20)}
\label{compare_dis_centralized}
\end{figure*}


Figure~\ref{size_control} depicts the empirical cumulative distribution of the CRs residing in certain sized clusters which are generated in 50 runs.
The centralized schemes form clusters which satisfy the requirement on cluster size strictly.
%When the desired size is 2, each generated cluster has two members, whereas when the desired size is 3, about 15\% CRs are formed into 2 node clusters.
As to ROSS-DFG and ROSS-DFA with size control mechanism, CR nodes averagely distributes in clusters whose sizes are 2, 3 and 4.
%When ROSS-3-DFA is applied, most number of CRs are in 3 node clusters, nevertheless, slightly less nodes are found in 2 node and 4 node clusters, there are also considerable number of singleton clusters.
%ROSS-3-DGA decreases the clusters sizes and results in more 2 node clusters, the second most CRs are found in 3 node clusters.
The sizes of clusters resulted from ROSS-DGA and ROSS-DFA are disperse, but appear to be better than SOC, i.e., the 50\% percentiles for ROSS-DGA, ROSS-DFA and SOC is 4.5, 5, and 5.5, and the 90\% percentiles for the three schemes are 8, 8, and 9.
%Figure~\ref{size_control} shows distributed clustering schemes are not able to control cluster sizes perfectly, but ROSS-DGA and ROSS-DFA eliminate the clusters whose size diverges largely with the desired one, \ie single node clusters and clusters with size of 13 and 14.

Note ROSS-DGA and ROSS-DFA with size control feature generate 10\%-20\% singleton clusters, which is due to the cluster pruning discussed in section~\ref{cluster_pruning}, whereas, without size control, only 3\% nodes are in singleton clusters.
When applying SOC, 10\% of nodes are in singleton clusters.

\subsubsection{Control Signalling Overhead}
%Different from the clustering schemes proposed in ~\cite{LIU_TMC11_2, clustering_globecom11}, 
%There are two phases for any variants of ROSS.
%Clusters are formed in the first phase, in the second phase, cluster membership is decided so that each node only resides in one cluster.
%Control message exchanges between CR nodes are involved in both phases.

In this section we compare the amount of control messages involved in different clustering schemes, \ie centralized scheme, ROC, and the variants of ROSS.
%In order to highlight the amount of control signalling only for clustering, 
We omit the control messaged involved in the process of neighbourhood discovery, which is the premise for the clustering schemes.
According to~\cite{complexity_aggregation_2011}, the message complexity is defined as the number of messages used by all nodes.
To have the same criterion to compare the overhead of signalling, we count \textit{the number of transmissions of control messages}, without distinguishing they are sent with broadcast or unicast.
This metric is synonymous with \textit{the number of updates} discussed in Section~\ref{ross}.

As to ROSS, the control messages are generated in both phases.
In the first phase, when a CR node decides itself to be the cluster head, it broadcasts a message containing its ID, cluster members and the set of CCCs in its cluster.
%As to ROSS with size control feature, there are same amount of cluster heads with ROSS without enabling size control feature, and the cluster head broadcasts the available channels of the pruned cluster.
In the second phase, a debatable node broadcasts its affiliation to inform its claiming clusters, then the $\text{CH}$s of the claiming clusters  broadcast message about the new cluster members if they are changed due to the debatable node's decision.
The total number of the decisions involved in cluster formation has been analysed in Theorem~\ref{clustering:theorem} and Section~\ref{clustering:phaseII:game} respectively.

Comparison scheme SOC involves three rounds of execution. 
In the first two rounds, every CR node maintains its own cluster and seek to integrate neighbouring clusters, or joins one of them.
The final clusters are obtained in the third round. 
In each round, every CR node is involved in comparisons and cluster mergers.
%Comparing with the second phase of ROSS, only debatable nodes to communicate with cluster heads to clarify their membership.
%The signalling overhead for centralized scheme comes from two processes, the the process of collecting information to the centralized controller, and the process that the controller spreads the clustering result to all the CR nodes.

The centralized scheme is conducted at the centralized control device, but it involves two phases of control message transmission.
The first phase is information aggregation, in which every CR node's channel availability and neighborhood is transmitted to the centralized controller.
The second phase is broadcasting, where the final clustering solution is disseminated over the network to every CR node.

We adopt the algorithm proposed in~\cite{Efficient_broadcasting_gathering_adhoc} to broadcast and gather information as the algorithm is simple and self-stabilizing.
This scheme needs building a backbone structure to support the communication, we use our generated cluster heads as the backbone and the debatable nodes as the gateway nodes between the backbone nodes.
As the backbone is built once and can support transmission for multiple times, the messages involved in the clustering process are not counted.
As to the process of information gathering, we assume that every cluster member sends the spectrum availability and its ID to its cluster head, which further forwards or the message to the controller.
The number of transmission for information gathering is $N$.
As to the process of dissemination, in an extreme situation where all the gateway nodes and the backbone nodes broadcast, the number of transmission will be $h + m$, where $h$ is the number of cluster heads, $m$ is number of debatable nodes, $d$ is the average number of demanding clusters for each debatable node.

The number of control messages which are involved in both ROSS and the centralized scheme is related with the number of debatable nodes.
Figure~\ref{percentage_overlapping_node} shows the percentage of debatable nodes when the CRN network becomes denser, from which we can obtain the value of $m$.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7\linewidth]{percentage_overlapping_node.pdf}
  \caption{The percentage of debatable nodes after phase I of ROSS.}\label{percentage_overlapping_node}
\end{figure}

%As we adopt a simplified communication model, node's transmission is not influenced by collision or interference, 
%Our simulation doesn't consider the behaviour in the physical layer, 


%Assume we use OSPF~\cite{BCJ10} to aggregate and disseminate information, then the best and worst complexity for is $\mathcal{O}(E)$, where $E$ is the number of edges in the graph which corresponds to the network.
%The minimum number of edges is $n-1$ when the nodes form a line and each node has at more two neighbours, and the maximum number is $N*(N-1)/2$ when the nodes form a complete graph.
%Thus the best message complexity of the centralized scheme is $\mathcal{O}{(N)}$ and the worst is $\mathcal{O}{(N^2)}$.

%1. update membership to form X1, 
%2. broadcast new X1, form new X2
%3. broadcast X3

%The complexity parameters are the number of nodes $n$ in network, number of clusters $h$.
The message complexity, quantitative analysis of the number of control messages involved in clustering, and the size of control messages are shown in Table \ref{tab_overhead}.
%$D(s)$: the maximum distance between centralized controller and CR users.
%
Figure~\ref{control_msg} shows the number of transmissions of SOC, the upper bound of the number of transmissions for ROSS, and the analytical number of transmissions of the centralised scheme. 
%Note that the curve for the centralized scheme is from theoretical analysis, which is $h+m+N$ as discussed beforehand.
%Note the overhead involved to construct the backbone (clusters) is not included.




%\begin{table}[hc]
%\centering
%\caption{Singalling overhead. Notations: $n$-number of CR nodes in CRN, $h$-number of cluster heads, $m$-number of debatable nodes, $c$-number of demanding clusters.}\label{tab_overhead}
%\begin{tabular}{|p{2.2 cm}|p{1.5 cm}|p{3.7 cm}|}
%\hline
% Scheme 					&   Number of transmissions 					& Content of message \\ \hline
% ROSS-DGA, ROSS-$\delta$-DGA 		&   $h+2*m^2c$ (upper bound)
%				& $ID_{H_C}$ and $K_C$ for $h+m^2c$ times, notification to join one cluster for $m^2c$ times					\\ \hline
% ROSS-DFA, ROSS-$\delta$-DFA 		&   $h+ 2m$	 (upper bound) 						& $ID_{H_C}$ and $K_C$ for $h+m$ times, notification to join one cluster for $m$ times	 					\\ \hline
% SOC 						&   $3*n$					& $\{K_i\}, i\in M\subseteq \texttt{Nb}_i$						\\ \hline
% Centralized				&	$n$						& $\{C\}$         	\\ 
% \hline
%\end{tabular}
%\end{table}

\begin{center}
\begin{table*}[ht]
\caption{Singalling overhead.}\label{tab_overhead}
{\small
\hfill{}
\begin{tabular}{|L{2.2 cm}|C{2.8 cm}|C{3.15 cm}|C{6.6 cm}|}
\hline
 Scheme 				&Message Complexity 	&   Quantitative number of messages 		& Content of message (size of message) 									\\ \hline
 ROSS-DGA, ROSS-$\delta$-DGA 	&$\mathcal{O}(N^3)$ (worst case)		&   $h+2*m^2d$ (upper bound)  				&   \multirow{2}{*}{\parbox{6.4cm}{Phase I: notification from cluster head (1 byte), new individual connectivity degree (1 byte);  Phase II: update of debatable nodes' affiliation (1 byte), claiming cluster$i$' new membership ($|C(i)|$ bytes)}}								\\ \cline{1-3}
 ROSS-DFA, ROSS-$\delta$-DFA 	&$\mathcal{O}(N)$ (worst case)		&   $h + 2m$	 (upper bound) 					& 	      												\\ \hline
 SOC 					&$\mathcal{O}(N)$		&   $3*N$									& $C(i)$ ($|C(i)|$ bytes), $K(C(i))$ ($|\mathcal{P}|$ bytes), $i\in \mathcal{N}$						\\ \hline
 Centralized			&$\mathcal{O}(N)$			&	$h + m + N$ (upper bound)~\cite{Efficient_broadcasting_gathering_adhoc}		& $\{C\}$ ($|C_i|*N$ bytes)        					\\ \hline
\end{tabular}
}
\hfill{}
\end{table*}
\end{center}
% centralized: $\mathcal{O}(D(s)+log N)$ + $\mathcal{O}(1+D(s)+log N)$



\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7\linewidth]{number_controlMsg.pdf}
  \caption{Number of control messages. Note the curves for ROSS-DGA and ROSS-DFA are the upper bounds of the number of messages, the curve of centralized scheme reflects an extreme situation.}
  \label{control_msg}
\end{figure}


\subsection{Comparison between Distributed Schemes}
In this section we investigate the performances of distributed clustering schemes in CRN with different network scales and densities.
The transmission range of CR is $A/10$, PR's transmission range is $A/5$.
The number of PU is 30.
%The number of CR is 100, 200 and 300, and the average number of neighbours of each CR is 9.5, 20, and 31.
We list some parameters of the simulation in the Table~\ref{Simulation_para}.


\begin{table}[ht]
\caption{}
\label{Simulation_para}
{\small
\hfill{}
\begin{tabular}{|L{3.7 cm}|C{1 cm}|C{1 cm}|C{1 cm}|}
\hline
Number of CRs			& 100 	&  200 					& 300 \\ \hline
Average num. of neighbours 	&9.5	&   20		& 31  \\ \hline
Desired size $\delta$ 	& 6	&   12 						& 20      \\ \hline
\end{tabular}
}
\hfill{}
\end{table}



\subsubsection{Number of CCCs per Non-singleton Clusters}

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{ccc_large_scale_color.pdf}
  \caption{Number of common channels of non-singleton clusters.}
  \label{ccc_large_scale}
\end{figure}

Figure~\ref{ccc_large_scale} illustrates the average number of CCCs of the non-singleton clusters.
It shows when $N=100$, variants of ROSS have 30\% less CCCs than SOC, but this gap is decreased significantly when $N$ is 200 and 300, \ie when $N=300$, number of CCCs achieved by ROSS variants (except for ROSS-$\delta$-DFA) is almost the same with that resulted from SOC.

This means SOC performs better in terms of the average number of CCCs per non-singleton clusters when network is sparse.
this is also observed in the evaluation in Section \ref{ccc_20} where $N=20$.
When the network becomes denser, even this metric favours SOC as discussed in the beginning of Section~\ref{performance}, ROSS-DGA achieves even more CCCs than SOC, and ROSS-DFA and ROSS-$\delta$-DGA increase the number of CCCs visibly.




\subsubsection{Survival Rate of Clusters with Increasing Primary Users}

In this part of simulation, we investigate the robustness of clusters by increasing the PUs working on certain channels.
 
Figure~\ref{singleton_clusters_100} illustrates the increasing trend of singleton clusters with the increase of PUs.
SOC generates around 10 more singleton clusters than the variants of ROSS, which accounts for 10\% of the total CR nodes.
We only show the average values of the variants of ROSS as their confidence intervals overlap.
%It can be seen that greedy algorithms result in slightly less singleton clusters than their counterparts.
%
Figure ~\ref{singleton_clusters_300} depicts a denser CRN where $N=300$.
SOC noticeably causes more singleton clusters than ROSS variants, except that ROSS-20-DFA results in more singleton clusters when PUs are few.
The reason is ROSS-20-DFA conducts cluster membership clarification for only once, which causes large number of singleton clusters.
ROSS-20-DGA increases the size of smaller clusters through debatable nodes' repeated updates thus drastically decreases the number of singleton clusters.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\linewidth]{survival_rate_100.pdf}
  \caption{Percentage of CRs which are not included in any clusters with the increasing number of primary users, $N=100$}
  \label{singleton_clusters_100}
\end{figure}
  
  \begin{figure}[!h]
    \centering
   \includegraphics[width=0.8\linewidth]{survival_rate_300.pdf}
  \caption{Percentage of CRs which are not included in any clusters with the increasing number of primary users, $N=300$}
  \label{singleton_clusters_300}
\end{figure}

From the Figure~\ref{singleton_clusters_100} and \ref{singleton_clusters_300}, we can conclude that the greedy versions of ROSS are slightly more robust than their counterpart variants of ROSS, and the clusters obtained from variants of ROSS are clearly more robust than SOC. 
%When the network is denser, the improvement on cluster sizes and robustness by the greedy search in the membership clarification phase is more obvious.


\subsubsection{Cluster Size Control}

The number of formed clusters is shown in Fig.~\ref{nClusters_largeNetwork}.
When the network scales up, the number of formed clusters by ROSS increases by smaller margin.
This result coincides with the analysis in Section~\ref{cluster_pruning}, that with ROSS, the number of formed clusters saturates when the network scales.
When the network becomes denser, more clusters are generated by SOC compared with ROSS variants.
To better understand the distribution of the sizes of formed clusters, we depict the cluster sizes with cumulative distribution.
In this group of evaluation, the number of PRs is 30.

\begin{figure}[!h]
  \centering
   \includegraphics[width=1\linewidth]{nClusters_largeNetwork.pdf}
  \caption{The number of formed clusters.}
  %, there are $x=6$ when $N=100$, $x=12$ when $N=200$, $x=21$ when $N=300$, which is around $2/3$ of the number of average neighbours.
  \label{nClusters_largeNetwork}
\end{figure}





%\begin{figure}[ht]
%\begin{center}
%%\centering
%\subfigure[100 CRs, 30 PRs]{\label{result1:1}\includegraphics[width=0.48\linewidth]{cdf_clusterSize_100.pdf}}
%\subfigure[300 CRns, 30 PRns]{\label{result1:2}\includegraphics[width=0.48\linewidth]{cdf_clusterSize_300.pdf}}
%\end{center} 
%\caption[Cluster sizes]{Distribution of CRs in clusters with different sizes} %{\subref{a}, \subref{b}, \subref{c}, \subref{d}}
%\label{result1}
%\end{figure}

Figures~\ref{cdf_clusterSize_100}~\ref{cdf_clusterSize_200}~\ref{cdf_clusterSize_300} illustrate the empirical cumulative distribution of CR nodes which reside in clusters with certain sizes in CRNs with different densities.
%The desired cluster sizes are 5, 8 and 13, which are around 45\% of the average number of neighbours in respective CRNs. 
When the variants of ROSS with size control feature are applied, the sizes of the most generated clusters are below $\delta$, and most of them are around the 50\% percentile.
%This means the size control feature effectively restricts the size of clusters, which is able to prevent the clusters from ungovernable growing with the increase of network density.
The sizes of clusters generated by ROSS-DGA and ROSS-DFA span a wider range than that with feature control feature.
We find that average number of neighbours is roughly equal with the 95\% percentile of the ROSS-DGA curve.
%But the range where most of the CRs (80\% which exclude the CR nodes residing in small and big clusters) resides centres a value, which is roughly the half of the average number of neighbours.
As to SOC, the 95\% percentiles are 36, 30, and 40.
Overviewing the three Figures, we can see ROSS-DGA and ROSS-DFA show similar behaviour on cluster sizes.
The clusters generated from SOC demonstrate strong divergence on cluster sizes.


\begin{figure*}[t]
\begin{multicols}{3}
    \includegraphics[width=\linewidth]{cdf_clusterSize_100.pdf}\par\caption{100 CRs, 30 PRs in network}\label{cdf_clusterSize_100}
    \includegraphics[width=\linewidth]{cdf_clusterSize_200.pdf}\par\caption{200 CRs, 30 PRs in network}\label{cdf_clusterSize_200}
    \includegraphics[width=\linewidth]{cdf_clusterSize_300.pdf}\par\caption{300 CRs, 30 PRs in network}\label{cdf_clusterSize_300}
\end{multicols}
\caption{Cumulative distribution of CRs residing in clusters with different sizes}
\label{cdf_100_200_300}
\end{figure*}


\section{Conclusion}
\label{conclusion}
In this paper we design a distributed clustering scheme with the singleton congestion model, which forms robust clusters against primary users' effect.
Through simulation and theoretical analysis, we find that distributed scheme achieves similar performance with centralized optimization in terms of cluster survival ratio and number of control messages.
%, which is important to form clusters which maintains unbroken to the greatest extent possible under primary users' activity.
This paper investigates the robust clustering problem in CRN extensively, and proves the NP hardness of this problem.
A Light weighted clustering scheme ROSS is proposed, on the basis of which, we propose the fast version scheme and the scheme which generate clusters with desired sizes.
These schemes outperform other distributed clustering scheme in terms of both cluster survival ratio and control overhead.

%The clusters resulted from ROSS-DGA and its faster version ROSS-DFA are less vulnerable compared with other distributed clustering schemes, and demonstrates similar survival rate with centralized scheme under primary users' influence.
%An light weighted cluster size control mechanism is contained in both ROSS-DGA and ROSS-DFA, which is advantageous for cooperative sensing and network operation with clusters.
%Furthermore, considerable less control messages are generated when compared with other clustering schemes.

The shortage of ROSS scheme is it doesn't generate big clusters whose sizes exceed the cluster head's neighbourhood.
This problem is attributed to fact that ROSS forms clusters on the basis of cluster head's neighbourhood, and doesn't involve interaction with the nodes outside its neighbourhood.
In the other way around, forming big cluster which extends out side of cluster head's neighbourhood has very limited applications, because multiple hop communication and coordination are required mange this kind of big clusters.








% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
%\section{Algorithm~\ref{alg0}}
\section*{}
\begin{algorithm}              % enter the algorithm environment
\caption{ROSS phase I: cluster head determination and initial cluster formation for Unclustered CR node $i$}          % give the algorithm a caption
\label{alg0} 
\DontPrintSemicolon
\SetAlgoLined
\KwIn{$d_j, g_j, j\in \text{Nb}_i\setminus \text{CHs}$. Empty sets $\tau_1,\tau_2$}
\KwResult{Returning 1 means $i$ is cluster head, then $d_j$ is set to 0, $j\in \text{Nb}_i\setminus \text{CHs}$. returning 0 means $i$ is not CH.}

\If{$\nexists j\in \text{Nb}_i\setminus \text{CHs}$, such that $d_i \geq d_j$}{
	return 1;
	}
\eIf{$\exists j\in \text{Nb}_i\setminus CHs$, such that $d_i > d_j$}{
	return 0;}{
	\If{$\nexists j\in \text{Nb}_i\setminus CHs$, such that $d_j == d_i$}{
	$\tau_1 \leftarrow j$
	}
}
\If{$\nexists j\in \tau_1$, such that $g_i \leq g_j$}{
	return 1;
	}
\eIf{$\exists j\in \tau_1$, such that $g_i < g_j$}{
	return 0;
	}
	{\If{$\nexists j\in \tau_1$, such that $g_j == g_i$}{
		$\tau_2\leftarrow j$
		}
	}
\If{$\texttt{ID}_i$ is smaller than any $\texttt{ID}_j$, $j\in \tau_2\setminus i$}{
	return 1;
	}
	{return 0;
	}
\end{algorithm}

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section*{Algorithm~\ref{alg_size_control_available_CCC}}
\section*{}
\begin{algorithm}               % enter the algorithm environment
\caption{ROSS phase I: cluster head guarantees the availability of CCC (use line 1) / cluster size control (use line 2)}          % give the algorithm a caption
\label{alg_size_control_available_CCC}
\DontPrintSemicolon
\SetAlgoLined
\KwIn{Cluster C, empty sets $\tau_1, \tau_2$}
\KwOut{Cluster C has at least one CCC, or satisfies the requirement on cluster size}
%\tcc*[r]{When to guarantee available CCCs, execute from line 1, when to control cluster size, execute from line 2}
\textbf{while} $K_C =\emptyset$ \textbf{do}\\
\While{$|C|> \delta$}{
	%calculate $\lambda = \min_{i\in C, i\neq H_C}(|K_{H_C}\cap K_i|)$;\\
	\eIf{$\exists$ only one $i\in C\setminus H_C$, $i = \argmin(|K_{H_C}\cap K_i|)$}{
			$C=C\setminus i$;
		}{
				$\exists$ multiple $i$ which satisfies $i = \argmin(|K_{H_C}\cap K_i|)$;\\ $\tau_1\leftarrow i$;		
		}
		
	\eIf{$\exists$ only one $i\in \tau_1$, $i = \argmax(|\cap_{j\in C\setminus i} K_j|-|\cap_{j\in C} K_j|)$}{
		$C=C\setminus i$;
		}{
			%$\exists$ multiple $i$ which satisfies $i = \argmax(|\cap_{j\in C\setminus i} K_j|-|\cap_{j\in C} K_j|)$;\\
			$C=C\setminus i$, where $i = \argmin_{i\in \tau_1} \texttt{ID}_i $
			%$\texttt{ID}_i$ is smaller than any $\texttt{ID}_j$, $j\in \tau_2\setminus i$;
	}
}
\end{algorithm}


\section*{}
\begin{algorithm}               % enter the algorithm environment
\caption{Debatable node $i$ decides its affiliation in phase II of ROSS}
%, chooses one claiming cluster to stay and leaves all the other claiming clusters}          % give the algorithm a caption,  cluster to settle
\label{alg4}
\DontPrintSemicolon
\SetAlgoLined
\KwIn{all claiming clusters $C\in S_i$}
\KwOut{one cluster $C\in S_i$, node $i$ notifies all its claiming clusters in $S_i$ about its affiliation decision.
}
\While{$i$ has not chosen the cluster, or $i$ has joined cluster $\tilde{C}$, but $\exists C'\in S_i, C'\neq \tilde{C}$, which has $|K_{C'\setminus i}|-|K_{C'}|<|K_{C\setminus i}|-|K_{C}|$}{
	\eIf{$\exists$ only one $C\in S_i$, $C = \argmin(|K_{C\setminus i}| - |K_C|)$}{
			return $C$;
		}{
				$\exists$ multiple $C\in S_i$ which satisfies $C = \argmin(|K_{C\setminus i}| - |K_C|)$;\\ 
				$\tau_1\leftarrow C$;		
		}
	\eIf{$\exists$ only one $C\in \tau_1$, $C = \argmax(K_{H_C}\cap K_i)$}{
			return $C$;
		}{
				$\exists$ multiple $C\in S_i$ which satisfies $C = \argmax(K_{H_C}\cap K_i)$;\\ 
				$\tau_2\leftarrow C$;		
		}
	\eIf{$\exists$ only one $C\in \tau_2$, $C = \argmin|C|)$}{
			return $C$;
		}{
				return $\argmin_{C\in \tau_2}\texttt{ID}_{H_C}$;\\
		}
		}
\end{algorithm}

% use section* for acknowledgment

\section*{Proof of Theorem~\ref{clustering:theorem}}
\label{proof_clustering:theorem}
\begin{proof}
%Note that the formed cluster can be a singleton cluster, \ie cluster size is 1.
We consider a CRN which can be represented as a connected graph.
To simplify the discussion, we assume the secondary users have unique individual connectivity degrees, and they have identical ID and social connectivity degrees.
This assumption is fair as the social connectivity degrees and node ID are used to break ties as shown in Algorithm~\ref{alg0}, when the individual connectivity degrees are unique, it is not necessary to use the former two metrics to break ties.

%we assume every node has at least one neighbour.
Assuming there exist some secondary users which are not included into any cluster, and node $\alpha$ is of this kind.
As node $\alpha$ is not contained in any cluster, there must be at least one node $\beta\in \text{Nb}_\alpha$, with $d_{\alpha} > d_{\beta}$.
Otherwise, node $\alpha$ is eligible to form a cluster.
If node $\beta$ becomes cluster head, node $\alpha$ is included.
If node node $\beta$ is not cluster head, i.e., $\beta$ is not in any cluster, we can repeat the previous analysis made on node $\alpha$, and deduce that node $\beta$ has at least one neighbouring node $\gamma$ with $d_{\gamma} < d_{\beta}$.
When no cluster head appears, this series of nodes with monotonically decreasing $d_i$ might continue to grow, and finally ceases when the individual connectivity degree is zero, or all secondary nodes are already in the series of nodes.
An example of the formed node series is shown as Figure~\ref{lemma1}.

\begin{figure}[ht!]
  \centering
\includegraphics[width=0.6\linewidth]{lemma1.pdf}
% \includegraphics{figure1.pdf}
	\caption{The node series discussed in the proof for Theorem~\ref{clustering:theorem}, the deduction begins from node $\alpha$}
	\label{lemma1}
\end{figure}


Now we find the node $\omega$ is in the end of this series.
As $\omega$ is the end node and does not have neighbouring nodes with bigger individual connectivity degree $D$, $\omega$ becomes cluster head and incorporate all its one-hop neighbours, including the node before it in the node series (here we assume that every new formed cluster has common channels).
After that, the node recruited into cluster will set its connection degree $D$ to zero, which enables the node further down in the list to become a cluster head.
In this way, all the nodes in the series are included in at least one cluster in an inverse sequence.
This result contradicts the initial assumption and proves the claim stated above.
Meanwhile, through this proof, we know that within at most $N$ steps, all nodes will become a part of certain clusters.
\end{proof}


%
\section*{Proof of Theorem~\ref{theorem1}}
\label{proof_theorem1}
\begin{proof}
We put the definition of weighted k-set packing problem here as it will be used in the analysis on the complexity of our problem.

\begin{mydef}
\label{def_kset_packing}
\textit{Weighted k-set packing.} 

Given a set $\mathcal{G}$ which contains finite number of positive integers, and a collection of set $\mathcal{Q}=\{s_1,s_2,\cdots,s_m\}$, where for each element $s_r, 1\leq r \leq m$, there is $s_r\subseteq \mathcal{G}$, $ 1\leq|s_r| \leq k$, and $s_r$ has an associated weight which is positive real number.
The question is whether exists a collection $\mathcal{S}\subseteq \mathcal{Q}$, where $\mathcal{S}$ contains only disjoint sets and the total weight of sets in $\mathcal{S}$ is greater than $\lambda$.
Weighted k-set packing is NP-hard when $k\geqslant 3$.~\cite{Computers_a_Intractability}
\end{mydef}


To prove the centralized clustering problem is of NP-hard, we reduce the NP-hard problem \textit{weighted k-set packing} to it to prove the former is as hard as the later.
%, which means centralized clustering in CRN problem is as hard as weighted k-set packing to be solved.
To complete the reduction, we need to conduct following two steps:
\begin{itemize}
\item step 1: Show there exists a polynomial algorithm $\sigma$, by which any instance $\mathcal{S}$ of a weighted k-set packing can be transformed to a clustering solution $\sigma(\mathcal{S})$ which complies with Definition~\ref{def_centralized_clustering}.
\item step 2: Show that $\mathcal{S}$ is a \textit{yes} instance of weighted k-set packing if and only if $\sigma(\mathcal{S})$ is a \textit{yes} instance for CRN clustering problem.
\end{itemize}

We continue to use the notation adopted in the problem definition in Section~\ref{centralized_opt}.
Let set $\mathcal{G}$ contain $N$ natural numbers which are from 1 to $N$.
$\mathcal{Q}$ is a collection of sets $\{s_1, s_2,\cdots s_m\}$, each set is composed with certain elements in $\mathcal{G}$.
Assume $\mathcal{S}\subseteq \mathcal{Q}$ is one instance of weighted k-set packing, and the sets in $\mathcal{S}$ are disjoint.
$\omega$ indicates the weight for each set $s$, $\omega:\mathcal{S}\rightarrow \mathbb{Z}^{+}$.
%As the number of common channel of clusters is always integer
%give new weight to them in following way



The polynomial algorithm $\sigma$ consists of three transformations.

\begin{itemize}
\item Given a collection $\mathcal{Q}$, on basis of which we construct a CRN.
We prepare $N$ CR nodes who are labelled from 1 to $N$.
We put the CR users on a 2 dimension space, and deem a pair of them can communicate if they appear in the same set in $\mathcal{Q}$.
We regard each set in $\mathcal{Q}$ is a cluster, whose number of CCCs equals to the weight of that set.


%There is one situation deserving extra explanation in the second transformation, we adopt one example to discuss it.
Assuming two sets in $\mathcal{Q}$ are $s_1=\{1,2\}$ and $s_2=\{1,2,3\}$, then their weights are 3 and 5 respectively.
%Their dummy sets are $s_1'=\{1,1,2,2\}$ and $s_2'=\{1,1,2,2,3,3,4,4\}$ and their new weights are 3 and 5 as before.
We find it is impossible to map the sets into clusters in the same time, because the number of CCCs of the cluster which bases on $s_1$ should be no less than the cluster which bases on $s_2$, as the latter has one extra node compared with the former cluster.
%When $s_1'$ and $s_2'$ are mapped to CRN respectively, there is contradiction between them, because the number of common channels of CR node group $\{1,1,2,2\}$ is smaller than that of $\{1,1,2,2,3,3,4,4\}$.
But as to any instance of the solution to the weighted k-set packing problem, this contradiction doesn't happen because the instance $\mathcal{S}$ contains only disjoint sets, thus at most only one set of $s_1$ and $s_2$ appears in $\mathcal{S}$.%, then we can safely delete the connections based on the deleted set from the CRN, and the contradiction is eliminated.

\item In the second step, we transform the instance $\mathcal{S}$ to $\mathcal{S'}$ by adding dummy elements into each set in $\mathcal{S}$.
For each set $s_i\in \mathcal{S}$, the elements in $s_i$ are duplicated, for instance, given $s_i=\{1, 4, 6\}$, the dummy set $s_i'$ is $\{1,1,4,4,6,6\}$.
%By doing this, we obtain the dummy sets and constitute the dummy instance $\mathcal{S}'$ based on $\mathcal{S}$.
The purpose of this transformation is to eliminate the set in $\mathcal{S}$, which has single element.
The weight of set is unchanged after this transformation, \ie $\omega(s_i)=\omega(s_i')$.
%After this transmission, there is no set with only one element.
This transformation requires $\sum_{s_i \in \mathcal{S}} |s_i|$ steps.

\item In this step, we transform the instance $\mathcal{S'}$ to a clustering solution for CRN.
We prepare a second pool of CR nodes which are identical with the CR nodes prepared in step 1, i.e., identical IDs and channel availabilities on them, we call these CR nodes as dummy nodes.
We locate these CR nodes besides the CR nodes with the sane IDs in the CRN built in step 1, and there is connection between the CR node and its dummy node (the one CR node and its dummy node can be seen as two transceivers at one node).
Because of the dummy nodes, the clustering solution which corresponds to $\mathcal{S'}$ doesn't have singleton cluster.
This transformation requires $2\cdot\sum_{s_i \in \mathcal{S}} |s_i|$ steps.
%
Afterwards, the CR node whose ID doesn't appear in any set in $\mathcal{S}$ becomes single node clusters, according to the definition of clustering problem in CRN, the number of CCCs in these single node clusters is 0.
These singleton clusters and the clusters in $\mathcal{S}$ constitute a clustering solution, and finding the singleton clusters requires at most $N$ steps.
%We deliberately locate them so that at least one CR node can communicate with the rest of them.
%map every element in $\mathcal{S}$ into one CR node, \ie each integer corresponds to one CR node, in particular, that integer becomes the CR node's ID.
%As $s'$ is dummy set which has duplicated elements, we also deploy the the same CR node twice.

An example is shown in Table~\ref{no_hard_proof_instance}.
\begin{table}[h!]
     \begin{center}
     \begin{tabular}{ p{3cm}  p{4.5cm} }
     \toprule
      $\mathcal{N}$  & $\{1,2,3,4,5\}$\\ 
    \cmidrule(r){1-1}\cmidrule(lr){2-2}     
      $\mathcal{Q}$  & $\{(1),(1, 5),(1,2,4),(2,3), (4)\}$\\ 
    \cmidrule(r){1-1}\cmidrule(lr){2-2}     
      Instance for Weighted k-set packing  & $\{(1),(2,3), (4)\}$\\ 
    \cmidrule(r){1-1}\cmidrule(lr){2-2}
      Instance with dummy elements
      & 
		$\{(1,1),(2,2,3,3), (4,4)\}$
      \\    \cmidrule(r){1-1}\cmidrule(lr){2-2}
	Instance for clustering solution (dashed circles are dummy nodes)
	  &
	  %\raisebox{-\totalheight}{\includegraphics[width=0.8\linewidth]{np_hard_proof_dummy.pdf}}      
	  \begin{minipage}{.3\textwidth}
      \includegraphics[width=0.8\linewidth]{np_hard_proof_dummy.pdf}
    \end{minipage}
	  \\       \bottomrule
      \end{tabular}
      \caption{}
      \label{no_hard_proof_instance}
      \end{center}
      \end{table}

%As to duplicated elements, we also map them into a CR node, thus there exist CR nodes with identical ID.
%As a result, these CR nodes constitute a collection of CR nodes, and they can not be called as a CRN yet as there are not communication links drawn between them.
%Connections in CRN under this context is decided by physical conditions, which means to have a communication link, the two CR nodes should have common channels and locate within each other's transmission range.
%We further assume that all CR nodes locate in a way that any two pair of nodes has potential to be connected if their IDs are in the same set in $\mathcal{S}$, the instance for weighted k-set packing.
%\item Mere isolated nodes don't constitute network, thus we add connections in CRN based on the sets in $\mathcal{S}'$ sequentially.
%For each set $s'\in \mathcal{S}'$, we add connection between two CR nodes if their IDs can be found in $s'$.
%No connection exists between two CR nodes if their IDs don't appear in the same set in $\mathcal{S}$.



%We complete the polynomial algorithm $\sigma$ for the the transformation so far.
%The number of common channels of cluster $f$ is non-increasing function of cluster size, while, the weight of set in weighted k-set packing problem doesn't have this property.
%In weighted k-set packing, the weight of a set with smaller size could be larger than a set with more elements.
%But this difference doesn't hinder the transformation because an instance of weighted k-set packing contains only disjoint sets in $\mathcal{G}$.

\end{itemize}

We have crossed the hurdle of finding one polynomial algorithm $\sigma$ to transform instance of weighted k-set packing to an instance for clustering in CRN.
Now we look into the step 2 in reduction.

As to an instance $\mathcal{S}$ for weighted k-set packing, the sum weights is identical to the sum of CCCs in the CRN mapped from $\mathcal{S'}$, even $\mathcal{S}$ contains set which only has one element.
Thus, when the instance $\mathcal{S}$ is one solution and its sum weights is greater than $\lambda$, in the CRN which is mapped from $\mathcal{S'}$, the summed number of CCCs of the clusters is greater than $\lambda$.
%
When there is no solution out of set $\mathcal{G}$ for weighted k-set packing,
%let's assume the maximum sum of weights of all instances is $\sum_{s_i\in \mathcal{S}}\omega(s_i)=\delta < \lambda$. 
%The dummy set of each $s_i\in \mathcal{S}$ is mapped to cluster of CR nodes.
%Definition of CRN clustering regulates that the number of common channels is 0 when the cluster has only one node.
%As to $|s_i|=1$, the mapped cluster has two nodes, with one of them is the dummy CR node.
%Then number of common channels is on longer 0 but equals to the weight of corresponding set $s_i$.
%%Meanwhile, the number of common channels of all the other single node cluster is 0.
the summed number of CCCs of the clusters in the mapped CRN is also smaller than $\lambda$.

Thus, weighted k-set packing can be reduced to centralized clustering problem in CRN, and we can say the latter problem is of NP-hard.
\end{proof}


\section*{Acknowledgment}

The authors would like to thank xxxx


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)


% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiographynophoto}{Di Li}
received BE and MS degrees in control engineering from Zhejiang University and Shaanxi University of Science and Technology respectively in China.
He worked with James Gross for his PhD in RWTH Aachen University since 2010.
\end{IEEEbiographynophoto}

% if you will not have a photo at all:
\begin{IEEEbiography}{Erwin Fang}
Biography text here.
Biography text here.
Biography text here.
Biography text here.
Biography text here.
Biography text here.
Biography text here.
Biography text here.
Biography text here.
Biography text here.
\end{IEEEbiography}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiography}{James Gross}
Biography text here.
Biography text here.
Biography text here.
Biography text here.
Biography text here.
Biography text here.
Biography text here.
Biography text here.
Biography text here.
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



\bibliographystyle{IEEEtran}
\bibliography{../backmatter/myrefs}

\end{document}


